!pip -q install pandas numpy scipy scikit-learn tqdm pynndescent matplotlib ipywidgets

import os, re, time, json, hashlib
from pathlib import Path

import numpy as np
import pandas as pd
import scipy.sparse as sp

from tqdm.auto import tqdm
from sklearn.preprocessing import normalize
from sklearn.manifold import SpectralEmbedding
from sklearn.metrics import normalized_mutual_info_score
from pynndescent import NNDescent

_UI_AVAILABLE = True
try:
    import ipywidgets as widgets
    from IPython.display import display
except Exception:
    _UI_AVAILABLE = False

class ProgressUI:
    def __init__(self, enabled: bool = True, show_log: bool = False):
        self.enabled = bool(enabled) and _UI_AVAILABLE
        self.show_log = bool(show_log)
        self._overall_total = 1
        self._overall_value = 0
        self._run_total = 1
        self._run_value = 0

        if not self.enabled:
            self.box = None
            return

        self.status = widgets.HTML(value="<b>Status:</b> idle")
        self.overall = widgets.IntProgress(value=0, min=0, max=1, description="Overall", bar_style="")
        self.runbar = widgets.IntProgress(value=0, min=0, max=1, description="Run", bar_style="")
        self._log = widgets.Output(layout={"border": "1px solid #ddd", "max_height": "160px", "overflow_y": "auto"}) if self.show_log else None

        items = [self.status, self.overall, self.runbar]
        if self._log is not None:
            items.append(self._log)

        self.box = widgets.VBox(items)
        display(self.box)

    def set_status(self, text: str):
        if self.enabled:
            self.status.value = f"<b>Status:</b> {text}"
        else:
            print(text)

    def log(self, text: str):
        if not self.enabled or self._log is None:
            return
        with self._log:
            print(text)

    def start_overall(self, total_steps: int, text: str = "starting..."):
        self._overall_total = int(max(1, total_steps))
        self._overall_value = 0
        if self.enabled:
            self.overall.max = self._overall_total
            self.overall.value = 0
            self.overall.bar_style = ""
        self.set_status(text)

    def step_overall(self, n: int = 1, text: str | None = None):
        self._overall_value = min(self._overall_total, self._overall_value + int(n))
        if self.enabled:
            self.overall.value = self._overall_value
        if text is not None:
            self.set_status(text)

    def start_run(self, total_steps: int, text: str = "run starting..."):
        self._run_total = int(max(1, total_steps))
        self._run_value = 0
        if self.enabled:
            self.runbar.max = self._run_total
            self.runbar.value = 0
            self.runbar.bar_style = ""
        self.set_status(text)

    def step_run(self, n: int = 1, text: str | None = None):
        self._run_value = min(self._run_total, self._run_value + int(n))
        if self.enabled:
            self.runbar.value = self._run_value
        if text is not None:
            self.set_status(text)

    def mark_done(self, text: str = "DONE"):
        if self.enabled:
            self.overall.bar_style = "success"
            self.runbar.bar_style = "success"
        self.set_status(text)

_MAX_COMPONENT = 140

def _safe_name(s: str, max_len: int = _MAX_COMPONENT) -> str:
    s = str(s).strip()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^A-Za-z0-9._\-]+", "_", s)
    s = re.sub(r"_+", "_", s)
    if not s:
        s = "x"
    return s[:max_len] if len(s) > max_len else s

def _now_str():
    return time.strftime("%Y%m%d_%H%M%S")

def _ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)
    return p

def _atomic_replace(tmp_path: Path, final_path: Path):
    final_path.parent.mkdir(parents=True, exist_ok=True)
    os.replace(str(tmp_path), str(final_path))

def _tmp_path(final_path: Path, ext: str | None = None) -> Path:
    """
    Make a short temp filename in the same directory, to avoid Drive filename limits.
    """
    ext = ext if ext is not None else final_path.suffix
    ts = int(time.time() * 1_000_000)
    # leading dot keeps it hidden; name stays short
    return final_path.with_name(f".tmp_{ts}_{os.getpid()}{ext}")

def _atomic_write_text(path: Path, text: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = _tmp_path(path, ".tmp")
    tmp.write_text(text, encoding="utf-8")
    _atomic_replace(tmp, path)

def _atomic_write_json(path: Path, obj: dict):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = _tmp_path(path, ".tmp")
    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    _atomic_replace(tmp, path)

def _read_json(path: Path) -> dict | None:
    try:
        if path.exists():
            return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        pass
    return None

def _atomic_write_df_csv(df: pd.DataFrame, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = _tmp_path(path, ".csv")
    df.to_csv(tmp, index=False, encoding="utf-8-sig")
    _atomic_replace(tmp, path)

def _safe_read_csv(path: Path) -> pd.DataFrame:
    try:
        if path.exists():
            return pd.read_csv(path, encoding="utf-8-sig")
    except Exception:
        pass
    return pd.DataFrame()

def _atomic_save_npz_array(path: Path, **arrays):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = _tmp_path(path, ".npz")
    np.savez_compressed(tmp, **arrays)
    _atomic_replace(tmp, path)

def _atomic_save_sparse_npz(path: Path, mat: sp.spmatrix):
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = _tmp_path(path, ".npz")
    sp.save_npz(tmp, mat)
    _atomic_replace(tmp, path)

def _make_seeds(base_seed: int, n: int):
    rng = np.random.RandomState(int(base_seed))
    return [int(x) for x in rng.randint(0, 2**31 - 1, size=int(n), dtype=np.int64).tolist()]

def _fingerprint(cfg: dict, *, features: list[str], knn: int, k: int, emb_dims: int) -> str:
    relevant = {
        "features": list(features),
        "knn": int(knn),
        "k": int(k),
        "emb_dims": int(emb_dims),
        "data": dict(cfg["DATA"]),
        "graph": dict(cfg["GRAPH"]),
        "seeds": dict(cfg["SEEDS"]),
        "mncut": dict(cfg["MNCUT"]),
    }
    blob = json.dumps(relevant, sort_keys=True, ensure_ascii=False).encode("utf-8")
    return hashlib.md5(blob).hexdigest()

def _feature_tag(feature_names: list[str], *, all_feature_names: list[str] | None = None) -> str:
    """
    Short, stable tag for filenames.
    - If all features selected -> ALL
    - Else -> F{n}_{hash8}
    """
    if all_feature_names is not None and set(feature_names) == set(all_feature_names):
        return "ALL"
    s = "||".join(sorted(feature_names))
    h = hashlib.md5(s.encode("utf-8")).hexdigest()[:8]
    return f"F{len(feature_names)}_{h}"

def load_verbs(verb_list_csv: Path):
    verbs = (
        pd.read_csv(verb_list_csv, header=None, encoding="utf-8-sig")[0]
        .astype(str).str.replace("\ufeff", "", regex=False).str.strip()
    )
    verbs = verbs[verbs != ""].tolist()
    vid = {v: i for i, v in enumerate(verbs)}
    return verbs, vid

def read_verb_context_matrix(path: Path, *, vid: dict, V: int,
                            verb_col: str, ctx_col: str,
                            use_col: str, min_count: float, chunk: int,
                            weight_transform: str):
    if not path.exists():
        raise FileNotFoundError(path)

    ctx2id = {}
    rows_parts, cols_parts, data_parts = [], [], []
    usecols = [verb_col, ctx_col, use_col]

    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding="utf-8-sig"),
                   desc=f"Reading {path.name}"):
        df = df.dropna()
        if df.empty:
            continue
        df = df[df[use_col] >= min_count]
        if df.empty:
            continue

        v = df[verb_col].astype(str).map(vid)
        m = v.notna()
        if not m.any():
            continue

        v = v[m].astype(np.int32).to_numpy()
        ctx_words = df.loc[m, ctx_col].astype(str).to_numpy()
        w = df.loc[m, use_col].to_numpy(np.float32)

        if weight_transform == "raw":
            pass
        elif weight_transform == "log1p":
            w = np.log1p(w).astype(np.float32)
        elif weight_transform == "sqrt":
            w = np.sqrt(w).astype(np.float32)
        else:
            raise ValueError(f"Unknown weight_transform: {weight_transform}")

        c = np.empty_like(v, dtype=np.int32)
        for i, cw in enumerate(ctx_words):
            j = ctx2id.get(cw)
            if j is None:
                j = len(ctx2id)
                ctx2id[cw] = j
            c[i] = j

        rows_parts.append(v); cols_parts.append(c); data_parts.append(w)

    if not rows_parts:
        raise RuntimeError(f"No usable edges in {path.name}.")

    rows = np.concatenate(rows_parts)
    cols = np.concatenate(cols_parts)
    data = np.concatenate(data_parts)

    X = sp.coo_matrix((data, (rows, cols)), shape=(V, len(ctx2id)), dtype=np.float32).tocsr()
    X.sum_duplicates()
    return X

def read_verb_verb_matrix(path: Path, *, vid: dict, V: int,
                          prefix_col: str, suffix_col: str,
                          use_col: str, min_count: float, chunk: int,
                          weight_transform: str):
    if not path.exists():
        raise FileNotFoundError(path)

    rows_parts, cols_parts, data_parts = [], [], []
    usecols = [prefix_col, suffix_col, use_col]

    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding="utf-8-sig"),
                   desc=f"Reading {path.name}"):
        df = df.dropna()
        if df.empty:
            continue
        df = df[df[use_col] >= min_count]
        if df.empty:
            continue

        r = df[prefix_col].astype(str).map(vid)
        c = df[suffix_col].astype(str).map(vid)
        m = r.notna() & c.notna()
        if not m.any():
            continue

        r = r[m].astype(np.int32).to_numpy()
        c = c[m].astype(np.int32).to_numpy()
        w = df.loc[m, use_col].to_numpy(np.float32)

        if weight_transform == "raw":
            pass
        elif weight_transform == "log1p":
            w = np.log1p(w).astype(np.float32)
        elif weight_transform == "sqrt":
            w = np.sqrt(w).astype(np.float32)
        else:
            raise ValueError(f"Unknown weight_transform: {weight_transform}")

        rows_parts.append(r); cols_parts.append(c); data_parts.append(w)

    if not rows_parts:
        raise RuntimeError(f"No usable edges in {path.name}.")

    rows = np.concatenate(rows_parts)
    cols = np.concatenate(cols_parts)
    data = np.concatenate(data_parts)

    A = sp.coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float32).tocsr()
    A.sum_duplicates()
    A.setdiag(0)
    A.eliminate_zeros()
    return A

def apply_verbverb_mode(A: sp.csr_matrix, mode: str):
    if mode == "out":
        return A
    if mode == "in":
        return A.T.tocsr()
    if mode == "both":
        return sp.hstack([A, A.T], format="csr")
    raise ValueError("VERBVERB_MODE must be out/in/both")

def default_feature_specs(base_dir: str):
    BASE = Path(base_dir)
    return {
        "verb-verb":             {"path": str(BASE / "Thaisum_verb-verb_pairs.csv"),             "kind": "verbverb"},
        "verb-(noun)-verb":      {"path": str(BASE / "Thaisum_verb-(noun)-verb_pairs.csv"),      "kind": "verbverb"},
        "verb-(verb)-verb":      {"path": str(BASE / "Thaisum_verb-(verb)-verb_pairs.csv"),      "kind": "verbverb"},
        "verb-(adjective)-verb": {"path": str(BASE / "Thaisum_verb-(adjective)-verb_pairs.csv"), "kind": "verbverb"},
        "verb-(adverb)-verb":    {"path": str(BASE / "Thaisum_verb-(adverb)-verb_pairs.csv"),    "kind": "verbverb"},
        "verb-(pronoun)-verb":   {"path": str(BASE / "Thaisum_verb-(pronoun)-verb_pairs.csv"),   "kind": "verbverb"},

        "noun-verb":             {"path": str(BASE / "Thaisum_noun-verb_pairs.csv"),             "kind": "verbctx", "verb_col": "verb", "ctx_col": "noun"},
        "adjective-verb":        {"path": str(BASE / "Thaisum_adjective-verb_pairs.csv"),        "kind": "verbctx", "verb_col": "verb", "ctx_col": "adjective"},
        "adverb-verb":           {"path": str(BASE / "Thaisum_adverb-verb_pairs.csv"),           "kind": "verbctx", "verb_col": "verb", "ctx_col": "adverb"},
        "pronoun-verb":          {"path": str(BASE / "Thaisum_pronoun-verb_pairs.csv"),          "kind": "verbctx", "verb_col": "verb", "ctx_col": "pronoun"},

        "verb-noun":             {"path": str(BASE / "Thaisum_verb-noun_pairs.csv"),             "kind": "verbctx", "verb_col": "verb", "ctx_col": "noun"},
        "verb-adjective":        {"path": str(BASE / "Thaisum_verb-adjective_pairs.csv"),        "kind": "verbctx", "verb_col": "verb", "ctx_col": "adjective"},
        "verb-adverb":           {"path": str(BASE / "Thaisum_verb-adverb_pairs.csv"),           "kind": "verbctx", "verb_col": "verb", "ctx_col": "adverb"},
        "verb-pronoun":          {"path": str(BASE / "Thaisum_verb-pronoun_pairs.csv"),          "kind": "verbctx", "verb_col": "verb", "ctx_col": "pronoun"},
    }

def build_graph_and_embedding(X: sp.csr_matrix, *, cfg, knn: int, emb_dims: int, ui: ProgressUI | None = None):
    if ui: ui.step_run(0, "Finding active verbs...")
    active = X.getnnz(axis=1) > 0
    active_ids = np.where(active)[0].astype(np.int32)
    X_act = X[active]
    n = X_act.shape[0]
    if n < int(cfg["GRAPH"]["MIN_ACTIVE_VERBS"]):
        raise RuntimeError(f"too_few_active (n={n})")

    if ui: ui.step_run(0, "Normalizing rows...")
    Xn = normalize(X_act, axis=1)

    k_eff = int(min(int(knn), n - 1))
    if k_eff < 1:
        raise RuntimeError("too_few_active_neighbors")

    if ui: ui.step_run(0, f"Building KNN graph (k={k_eff})...")
    nn = NNDescent(
        Xn,
        n_neighbors=int(k_eff + 1),
        metric=str(cfg["GRAPH"]["KNN_METRIC"]),
        random_state=int(cfg["SEEDS"]["EMB_RANDOM_STATE"]),
        n_jobs=int(cfg["GRAPH"]["N_JOBS"]),
    )
    knn_idx, knn_dist = nn.neighbor_graph
    knn_sim = 1.0 - knn_dist

    I = np.repeat(np.arange(n, dtype=np.int32), k_eff)
    J = knn_idx[:, 1:k_eff+1].reshape(-1).astype(np.int32)
    S = knn_sim[:, 1:k_eff+1].reshape(-1).astype(np.float32)

    pos = S > float(cfg["GRAPH"]["SIM_MIN"])
    I, J, S = I[pos], J[pos], S[pos]

    if ui: ui.step_run(0, "Building symmetric affinity matrix...")
    W = sp.coo_matrix((S, (I, J)), shape=(n, n), dtype=np.float32).tocsr()
    if cfg["GRAPH"]["MAKE_SYMMETRIC"]:
        W = (W + W.T).tocsr()
    W.sum_duplicates()

    W_diag0 = W.copy().tocsr()
    W_diag0.setdiag(0)
    W_diag0.eliminate_zeros()

    diag_eps = float(cfg["GRAPH"]["DIAG_EPS"])
    W_emb = (W + sp.eye(n, dtype=np.float32) * diag_eps).tocsr()

    emb_dim_used = int(min(int(emb_dims), n - 2))
    if emb_dim_used < 2:
        raise RuntimeError(f"n too small for embedding: n={n}, emb_dim_used={emb_dim_used}")

    if ui: ui.step_run(0, f"Spectral embedding (dim={emb_dim_used})...")
    Z = SpectralEmbedding(
        n_components=emb_dim_used,
        affinity="precomputed",
        random_state=int(cfg["SEEDS"]["EMB_RANDOM_STATE"]),
    ).fit_transform(W_emb)

    Z = normalize(Z, axis=1)
    return active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used

def _load_embed_cache(embed_npz: Path, wdiag_npz: Path, expected_fp: str):
    if not (embed_npz.exists() and wdiag_npz.exists()):
        return None
    meta = _read_json(embed_npz.with_suffix(".meta.json"))
    if not meta or meta.get("fingerprint") != expected_fp:
        return None
    try:
        npz = np.load(embed_npz, allow_pickle=False)
        active_ids = npz["active_ids"].astype(np.int32)
        Z = npz["Z"].astype(np.float32)
        knn_idx = npz["knn_idx"].astype(np.int32)
        knn_sim = npz["knn_sim"].astype(np.float32)
        emb_dim_used = int(npz["emb_dim_used"][0])
        W_diag0 = sp.load_npz(wdiag_npz).tocsr()
        return active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used
    except Exception:
        return None

def _save_embed_cache(embed_npz: Path, wdiag_npz: Path, *, fp: str,
                      active_ids, Z, knn_idx, knn_sim, emb_dim_used, W_diag0):
    _atomic_save_npz_array(
        embed_npz,
        active_ids=np.asarray(active_ids, dtype=np.int32),
        Z=np.asarray(Z, dtype=np.float32),
        knn_idx=np.asarray(knn_idx, dtype=np.int32),
        knn_sim=np.asarray(knn_sim, dtype=np.float32),
        emb_dim_used=np.asarray([int(emb_dim_used)], dtype=np.int32),
    )
    _atomic_save_sparse_npz(wdiag_npz, W_diag0)
    _atomic_write_json(embed_npz.with_suffix(".meta.json"), {"fingerprint": fp})

def mncut_discretize(Vk, *, k, seed, cfg):
    hp = cfg["MNCUT"]
    Vk = np.asarray(Vk, dtype=np.float64)
    n, kk = Vk.shape
    if kk != int(k):
        raise ValueError("MNCut discretize expects Vk shape (n, k) where k == n_clusters")

    Vn = Vk / (np.linalg.norm(Vk, axis=1, keepdims=True) + 1e-12)

    rng = np.random.RandomState(int(seed))
    A = rng.normal(size=(int(k), int(k)))
    Q, _ = np.linalg.qr(A)
    R = Q

    labels_prev = None
    for _it in range(int(hp["MAX_ITER"])):
        Y = Vn @ R
        labels = np.argmax(Y, axis=1).astype(np.int32)

        counts = np.bincount(labels, minlength=int(k))
        empties = np.where(counts == 0)[0]
        if empties.size:
            part = np.partition(Y, -2, axis=1)
            margin = part[:, -1] - part[:, -2]
            order = np.argsort(margin)
            used = set()
            ptr = 0
            for ek in empties.tolist():
                while ptr < n and order[ptr] in used:
                    ptr += 1
                ridx = int(order[ptr]) if ptr < n else int(rng.randint(0, n))
                used.add(ridx)
                labels[ridx] = int(ek)

        if labels_prev is not None:
            change = float(np.mean(labels != labels_prev))
            if change <= float(hp["TOL"]):
                break
        labels_prev = labels.copy()

        E = np.zeros((n, int(k)), dtype=np.float64)
        E[np.arange(n), labels] = 1.0

        M = E.T @ Vn
        U, _, Vt = np.linalg.svd(M, full_matrices=False)
        R = Vt.T @ U.T

    return labels.astype(np.int32)

def _export_clusters_one_csv(
    out_dir: Path,
    *,
    verbs: list[str],
    V: int,
    active_ids: np.ndarray,
    seeds: list[int],
    labels_npz_fn,
    filename: str = "clusters.csv",
):
    out_dir = _ensure_dir(out_dir)
    clusters_csv = out_dir / filename

    df = pd.DataFrame({"verb": verbs})

    for i, seed in enumerate(seeds):
        p = labels_npz_fn(int(seed))
        if not p.exists():
            raise RuntimeError(f"Missing labels checkpoint for seed={seed}: {p}")

        z = np.load(p, allow_pickle=False)
        labels_active = z["labels"].astype(np.int32)

        full_labels = np.full(V, -1, dtype=np.int32)
        full_labels[active_ids] = labels_active

        df[f"cluster_{i}"] = full_labels

    _atomic_write_df_csv(df, clusters_csv)

def _assemble_matrix(feature_names: list[str], mats: dict[str, sp.csr_matrix]):
    blocks = [mats[f] for f in feature_names]
    return blocks[0] if len(blocks) == 1 else sp.hstack(blocks, format="csr")

def run_one_subset_resumable(
    *,
    out_dir: Path,
    verbs: list[str],
    V: int,
    X: sp.csr_matrix,
    feature_names: list[str],
    cfg: dict,
    knn: int,
    n_clusters: int,
    emb_dims: int,
    feats_tag_override: str | None = None,
    ui: ProgressUI | None = None
) -> dict:
    out_dir = _ensure_dir(out_dir)

    feats_tag = feats_tag_override or _feature_tag(feature_names)
    feats_tag = _safe_name(feats_tag)

    run_id = _safe_name(f"{cfg['IO']['RUN_STAMP']}_mncut_knn{knn}_k{n_clusters}_emb{emb_dims}_{feats_tag}")

    fp = _fingerprint(cfg, features=feature_names, knn=knn, k=n_clusters, emb_dims=emb_dims)

    embed_npz = out_dir / "_cache" / f"{run_id}__EMBED.npz"
    wdiag_npz = out_dir / "_cache" / f"{run_id}__Wdiag0.npz"
    state_json = out_dir / "_cache" / f"{run_id}__STATE.json"
    seed_metrics_csv = out_dir / "_cache" / f"{run_id}__seed_metrics.csv"
    done_json = out_dir / "_cache" / f"{run_id}__DONE.json"

    n_seeds = int(cfg["SEEDS"]["N_SEED_RUNS"])
    seeds = _make_seeds(int(cfg["SEEDS"]["BASE_SEED"]), n_seeds)

    labels_dir = out_dir / "_cache" / "_labels" / run_id
    labels_dir.mkdir(parents=True, exist_ok=True)

    def labels_npz(seed: int) -> Path:
        return labels_dir / f"labels_seed{seed}.npz"

    active_ids_fast = np.where(X.getnnz(axis=1) > 0)[0].astype(np.int32)

    if done_json.exists():
        saved = _read_json(done_json) or {}
        if saved.get("fingerprint") == fp:
            clusters_csv = out_dir / "clusters.csv"
            if not clusters_csv.exists():
                missing = [s for s in seeds if not labels_npz(int(s)).exists()]
                if missing:
                    raise RuntimeError(f"DONE.json exists but missing label checkpoints for seeds: {missing}")
                _export_clusters_one_csv(
                    out_dir,
                    verbs=verbs, V=V, active_ids=active_ids_fast,
                    seeds=[int(s) for s in seeds],
                    labels_npz_fn=labels_npz,
                    filename="clusters.csv",
                )

            df_seed = _safe_read_csv(seed_metrics_csv)
            if not df_seed.empty:
                return _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg)

    st = _read_json(state_json)
    if st and st.get("fingerprint") and st.get("fingerprint") != fp:
        raise RuntimeError(
            "Found existing checkpoints for this run_id, but config changed.\n"
            "Use a new OUT_DIR or change RUN_STAMP (or delete _cache for this run_id)."
        )

    if ui:
        ui.start_run(2 + n_seeds, f"Run: {run_id}")

    cached = _load_embed_cache(embed_npz, wdiag_npz, fp)
    if cached is not None:
        active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used = cached
        if ui: ui.step_run(1, "Loaded cached embedding.")
    else:
        active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used = build_graph_and_embedding(
            X, cfg=cfg, knn=knn, emb_dims=emb_dims, ui=ui
        )
        _save_embed_cache(embed_npz, wdiag_npz, fp=fp,
                          active_ids=active_ids, Z=Z,
                          knn_idx=knn_idx, knn_sim=knn_sim,
                          emb_dim_used=emb_dim_used, W_diag0=W_diag0)
        if ui: ui.step_run(1, "Saved embedding cache.")

    n = Z.shape[0]
    k_used = int(min(int(n_clusters), n - 1))
    if k_used < 2:
        raise RuntimeError("k_used<2")
    if k_used > Z.shape[1]:
        raise RuntimeError(f"MNCut needs emb_dims >= k_used (got emb={Z.shape[1]}, k_used={k_used})")

    _atomic_write_json(state_json, {
        "run_id": run_id,
        "fingerprint": fp,
        "features": list(feature_names),
        "knn": int(knn),
        "k": int(n_clusters),
        "k_used": int(k_used),
        "emb_dims": int(emb_dims),
        "emb_dim_used": int(emb_dim_used),
        "n_seeds": int(n_seeds),
        "seeds": list(seeds),
        "updated": _now_str(),
    })

    df_seed = _safe_read_csv(seed_metrics_csv)
    done_seeds = set(df_seed["seed"].astype(int).tolist()) if ("seed" in df_seed.columns and not df_seed.empty) else set()

    labels_list = []
    for s in seeds:
        p = labels_npz(int(s))
        if p.exists():
            try:
                z = np.load(p, allow_pickle=False)
                labels_list.append(z["labels"].astype(np.int32))
            except Exception:
                pass

    done_seeds = {int(s) for s in done_seeds if labels_npz(int(s)).exists()}
    remaining = [s for s in seeds if int(s) not in done_seeds]

    if ui: ui.step_run(0, f"Seeds remaining: {len(remaining)}/{len(seeds)}")

    Zk = normalize(Z[:, :k_used], axis=1)  # compute once

    for si, seed in enumerate(remaining, start=1):
        if ui: ui.step_run(0, f"MNCut seed {si}/{len(remaining)}...")

        labels = mncut_discretize(Zk, k=k_used, seed=int(seed), cfg=cfg).astype(np.int32)

        _atomic_save_npz_array(labels_npz(int(seed)), labels=labels)

        purity = _neighbor_purity_from_knn(knn_idx, knn_sim, labels)
        Q = _modularity_Q(W_diag0, labels)
        ncut = _multiway_ncut(W_diag0, labels)
        row = {
            "run_id": run_id,
            "seed": int(seed),
            "active_verbs": int(n),
            "k_used": int(k_used),
            "emb_dim_used": int(emb_dim_used),
            "neighbor_purity": float(purity),
            "modularity_Q": float(Q),
            "ncut": float(ncut),
            "score_purity_plus_Q": float(purity + Q),
        }
        df_seed = pd.concat([df_seed, pd.DataFrame([row])], ignore_index=True)
        df_seed = df_seed.sort_values(["seed"]).reset_index(drop=True)
        _atomic_write_df_csv(df_seed, seed_metrics_csv)

        labels_list.append(labels.copy())
        if ui: ui.step_run(1)

    nmi_mean, nmi_std = _nmi_stability(labels_list, cfg)

    _atomic_write_json(done_json, {"done": True, "when": _now_str(), "fingerprint": fp})

    _export_clusters_one_csv(
        out_dir,
        verbs=verbs,
        V=V,
        active_ids=active_ids,
        seeds=[int(s) for s in seeds],
        labels_npz_fn=labels_npz,
        filename="clusters.csv",
    )

    return _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg, nmi_mean, nmi_std)

def _neighbor_purity_from_knn(knn_idx, knn_sim, labels):
    n = labels.shape[0]
    pur = np.zeros(n, dtype=np.float64)
    for i in range(n):
        inds = knn_idx[i]
        sims = knn_sim[i]
        m = inds != i
        inds = inds[m]; sims = sims[m]
        if inds.size == 0:
            pur[i] = np.nan
            continue
        sims = np.maximum(sims, 0.0)
        tot = float(sims.sum())
        if tot <= 0:
            pur[i] = np.nan
            continue
        inside = float(sims[labels[inds] == labels[i]].sum())
        pur[i] = inside / tot
    return float(np.nanmean(pur))

def _modularity_Q(W_diag0, labels):
    deg = np.asarray(W_diag0.sum(axis=1)).ravel().astype(np.float64)
    m = float(W_diag0.sum() / 2.0)
    if m <= 0:
        return np.nan
    two_m = 2.0 * m
    Q = 0.0
    for c in np.unique(labels):
        mask = (labels == c)
        if mask.sum() == 0:
            continue
        vol_c = float(deg[mask].sum())
        internal_twice = float(W_diag0[mask][:, mask].sum())
        Q += (internal_twice / two_m) - (vol_c / two_m) ** 2
    return float(Q)

def _multiway_ncut(W_diag0, labels):
    deg = np.asarray(W_diag0.sum(axis=1)).ravel().astype(np.float64)
    ncut = 0.0
    for c in np.unique(labels):
        mask = (labels == c)
        if not np.any(mask):
            continue
        vol = float(deg[mask].sum())
        if vol <= 0:
            continue
        internal_twice = float(W_diag0[mask][:, mask].sum())
        cut = vol - internal_twice
        ncut += cut / vol
    return float(ncut)

def _nmi_stability(labels_list, cfg):
    S = len(labels_list)
    if S < 2:
        return (np.nan, np.nan)

    max_pairs = cfg["METRICS"]["NMI_MAX_PAIRS"]
    pairs = [(i, j) for i in range(S) for j in range(i+1, S)]

    if max_pairs is not None and len(pairs) > int(max_pairs):
        rng = np.random.RandomState(int(cfg["SEEDS"]["BASE_SEED"]) + 999)
        idx = rng.choice(len(pairs), size=int(max_pairs), replace=False)
        pairs = [pairs[t] for t in idx.tolist()]

    vals = []
    for i, j in pairs:
        vals.append(normalized_mutual_info_score(labels_list[i], labels_list[j], average_method="arithmetic"))

    vals = np.array(vals, dtype=np.float64)
    return float(np.mean(vals)), float(np.std(vals))

def _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg, nmi_mean=None, nmi_std=None):
    def _mean(x): return float(np.nanmean(x))
    def _std(x): return float(np.nanstd(x))

    if nmi_mean is None or nmi_std is None:
        nmi_mean = np.nan
        nmi_std = np.nan

    row = {
        "run_id": str(run_id),
        "features": " | ".join(feature_names),
        "n_features": int(len(feature_names)),
        "knn": int(knn),
        "k": int(n_clusters),
        "emb_dims": int(emb_dims),
        "active_verbs": int(df_seed["active_verbs"].iloc[0]) if (not df_seed.empty and "active_verbs" in df_seed.columns) else np.nan,
        "n_seeds": int(len(df_seed)),

        "neighbor_purity_mean": _mean(df_seed["neighbor_purity"]),
        "neighbor_purity_std": _std(df_seed["neighbor_purity"]),
        "modularity_Q_mean": _mean(df_seed["modularity_Q"]),
        "modularity_Q_std": _std(df_seed["modularity_Q"]),
        "ncut_mean": _mean(df_seed["ncut"]),
        "ncut_std": _std(df_seed["ncut"]),
        "score_mean": _mean(df_seed["score_purity_plus_Q"]),
        "score_std": _std(df_seed["score_purity_plus_Q"]),

        "nmi_stability_mean": float(nmi_mean),
        "nmi_stability_std": float(nmi_std),

        "fingerprint": str(fp),
        "run_stamp": str(cfg["IO"]["RUN_STAMP"]),
        "updated": _now_str(),
    }
    return row

def make_config(
    *,
    base_dir: str,
    out_dir: str,
    run_stamp: str | None = None,
    n_seed_runs: int = 10,
    base_seed: int = 0,
    emb_random_state: int = 0,
    use_col: str = "total_occurrences",
    min_count: float = 0.0,
    chunk: int = 1_000_000,
    verbverb_mode: str = "both",
    weight_transform: str = "raw",
    graph: dict | None = None,
    mncut_hp: dict | None = None,
    nmi_max_pairs: int | None = None,
    ui: dict | None = None,
):
    _graph = dict(
        KNN_METRIC="cosine",
        N_JOBS=-1,
        SIM_MIN=0.0,
        MAKE_SYMMETRIC=True,
        DIAG_EPS=1e-6,
        MIN_ACTIVE_VERBS=5,
    )
    if graph:
        _graph.update(graph)

    _mncut = dict(MAX_ITER=50, TOL=1e-6)
    if mncut_hp:
        _mncut.update(mncut_hp)

    if ui is None:
        ui = {"ENABLE": True, "SHOW_LOG": False}

    cfg = {
        "UI": ui,
        "IO": {
            "OUT_DIR": out_dir,
            "RUN_STAMP": run_stamp,
        },
        "DATA": {
            "VERB_LIST_CSV": str(Path(base_dir) / "Wiktionary_Thai_verb_26122025.csv"),
            "USE_COL": use_col,
            "MIN_COUNT": float(min_count),
            "CHUNK": int(chunk),
            "VERBVERB_MODE": verbverb_mode,
            "WEIGHT_TRANSFORM": weight_transform,
        },
        "SEEDS": {
            "BASE_SEED": int(base_seed),
            "N_SEED_RUNS": int(n_seed_runs),
            "EMB_RANDOM_STATE": int(emb_random_state),
        },
        "GRAPH": _graph,
        "MNCUT": _mncut,
        "METRICS": {"NMI_MAX_PAIRS": nmi_max_pairs},
    }
    return cfg

def load_selected_feature_mats(*, base_dir: str, vid: dict, V: int, cfg: dict,
                               feature_on: dict[str, bool]) -> dict[str, sp.csr_matrix]:
    specs = default_feature_specs(base_dir)
    chosen = [k for k, v in feature_on.items() if bool(v)]
    chosen = [k for k in chosen if k in specs]
    if not chosen:
        raise RuntimeError("No features selected. Set at least one FEATURE_ON[name]=True")

    mats = {}
    for f in chosen:
        spec = specs[f]
        path = Path(spec["path"])
        if not path.exists():
            raise FileNotFoundError(f"Missing feature file: {path}")

        kind = spec["kind"]
        if kind == "verbverb":
            A = read_verb_verb_matrix(
                path,
                vid=vid, V=V,
                prefix_col=spec.get("prefix_col", "prefix"),
                suffix_col=spec.get("suffix_col", "suffix"),
                use_col=cfg["DATA"]["USE_COL"],
                min_count=cfg["DATA"]["MIN_COUNT"],
                chunk=cfg["DATA"]["CHUNK"],
                weight_transform=cfg["DATA"]["WEIGHT_TRANSFORM"],
            )
            X = apply_verbverb_mode(A, cfg["DATA"]["VERBVERB_MODE"])
        elif kind == "verbctx":
            X = read_verb_context_matrix(
                path,
                vid=vid, V=V,
                verb_col=spec["verb_col"],
                ctx_col=spec["ctx_col"],
                use_col=cfg["DATA"]["USE_COL"],
                min_count=cfg["DATA"]["MIN_COUNT"],
                chunk=cfg["DATA"]["CHUNK"],
                weight_transform=cfg["DATA"]["WEIGHT_TRANSFORM"],
            )
        else:
            raise ValueError(f"Unknown feature kind: {kind}")

        X = X.tocsr()
        X.sum_duplicates()
        mats[f] = X

    return mats

def run_experiment(
    *,
    base_dir: str,
    out_dir: str,
    feature_on: dict[str, bool],
    knn: int,
    n_clusters: int,
    emb_dims: int,
    cfg: dict
) -> pd.DataFrame:
    ui_cfg = cfg.get("UI", {})
    ui = ProgressUI(enabled=bool(ui_cfg.get("ENABLE", True)), show_log=bool(ui_cfg.get("SHOW_LOG", False)))

    out_path = _ensure_dir(Path(out_dir))

    stamp_file = out_path / ".RUN_STAMP.txt"
    if cfg["IO"]["RUN_STAMP"]:
        run_stamp = str(cfg["IO"]["RUN_STAMP"])
        if not stamp_file.exists():
            _atomic_write_text(stamp_file, run_stamp)
    else:
        if stamp_file.exists():
            run_stamp = stamp_file.read_text(encoding="utf-8").strip()
        else:
            run_stamp = _now_str()
            _atomic_write_text(stamp_file, run_stamp)
    cfg["IO"]["RUN_STAMP"] = run_stamp

    ui.set_status("Loading verbs...")
    verbs, vid = load_verbs(Path(cfg["DATA"]["VERB_LIST_CSV"]))
    V = len(verbs)
    print("Loaded verbs:", V)

    ui.set_status("Loading selected feature matrices...")
    mats = load_selected_feature_mats(base_dir=base_dir, vid=vid, V=V, cfg=cfg, feature_on=feature_on)
    feature_names = list(mats.keys())
    print("Selected features:", feature_names)

    ui.set_status("Assembling matrix...")
    X = _assemble_matrix(feature_names, mats)

    ui.start_overall(1, "Running MNCut (resumable)...")

    all_features = list(default_feature_specs(base_dir).keys())
    feats_tag_override = _feature_tag(feature_names, all_feature_names=all_features)

    summary = run_one_subset_resumable(
        out_dir=out_path,
        verbs=verbs,
        V=V,
        X=X,
        feature_names=feature_names,
        cfg=cfg,
        knn=int(knn),
        n_clusters=int(n_clusters),
        emb_dims=int(emb_dims),
        feats_tag_override=feats_tag_override,
        ui=ui
    )

    results_csv = out_path / "master_results.csv"
    df_existing = _safe_read_csv(results_csv)
    if not df_existing.empty and "run_id" in df_existing.columns:
        df_existing = df_existing[df_existing["run_id"].astype(str) != str(summary["run_id"])].copy()
        df_out = pd.concat([df_existing, pd.DataFrame([summary])], ignore_index=True)
    else:
        df_out = pd.DataFrame([summary])

    _atomic_write_df_csv(df_out, results_csv)

    ui.step_overall(1, "Finished.")
    ui.mark_done(f"DONE. Exported: {results_csv} and {out_path / 'clusters.csv'}")
    return df_out

from google.colab import drive
drive.mount("/content/drive")

BASE_DIR = "/content/drive/MyDrive/Colab_Datasets/VV"
OUT_DIR  = f"{BASE_DIR}/VV_clusters" ###

KNN = 10
N_CLUSTERS = 100
EMB_DIMS = 200

N_SEED_RUNS = 100
BASE_SEED = 0

FEATURE_ON = {
    "verb-verb": True, ###
    "verb-(noun)-verb": False, ###
    "verb-(verb)-verb": False, ###
    "verb-(adjective)-verb": False, ###
    "verb-(adverb)-verb": False, ###
    "verb-(pronoun)-verb": False, ###
    "noun-verb": False, ###
    "adjective-verb": False, ###
    "adverb-verb": False, ###
    "pronoun-verb": False, ###
    "verb-noun": False, ###
    "verb-adjective": False, ###
    "verb-adverb": False, ###
    "verb-pronoun": False, ###
}

MNCUT_HP = {"MAX_ITER": 50, "TOL": 1e-6}
UI = {"ENABLE": True, "SHOW_LOG": False}

cfg = make_config(
    base_dir=BASE_DIR,
    out_dir=OUT_DIR,
    run_stamp=None,
    n_seed_runs=N_SEED_RUNS,
    base_seed=BASE_SEED,
    emb_random_state=0,
    min_count=0.0,
    chunk=1_000_000,
    verbverb_mode="both", ###
    weight_transform="raw",
    mncut_hp=MNCUT_HP,
    nmi_max_pairs=None,
    ui=UI,
)

df_results = run_experiment(
    base_dir=BASE_DIR,
    out_dir=OUT_DIR,
    feature_on=FEATURE_ON,
    knn=KNN,
    n_clusters=N_CLUSTERS,
    emb_dims=EMB_DIMS,
    cfg=cfg
)

display(df_results)
