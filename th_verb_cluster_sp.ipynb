{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Thai Semantic Verb Clustering from Selectional Preference**"
      ],
      "metadata": {
        "id": "vhffXPBeYEoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Co-occurrence pairs creator**"
      ],
      "metadata": {
        "id": "qrCLGJbeZTRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!pip -q install -U huggingface_hub datasets\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/thaisum_parquet\")\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"pythainlp/thaisum\",\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=str(SAVE_DIR),\n",
        "    local_dir_use_symlinks=False,\n",
        "    allow_patterns=[\"data/*.parquet\"]\n",
        ")\n",
        "\n",
        "print(\"Saved to:\", SAVE_DIR)\n",
        "print(\"Parquet files found:\", len(list((SAVE_DIR / \"data\").glob(\"*.parquet\"))))\n",
        "\n",
        "!pip -q install datasets pyahocorasick tqdm pandas numpy\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import ahocorasick\n",
        "from collections import defaultdict\n",
        "\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/thaisum_parquet\")\n",
        "DATA_DIR = SAVE_DIR / \"data\"\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/Colab_Datasets/VV\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VERB_CSV_PATH = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/Wiktionary_Thai_verb_26122025.csv\")\n",
        "NOUN_CSV_PATH = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/Wiktionary_Thai_noun_27122025.csv\")\n",
        "\n",
        "ADVERB_CSV_PATH    = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/Wiktionary_Thai_adverb_02012026.csv\")\n",
        "ADJECTIVE_CSV_PATH = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/Wiktionary_Thai_adjective_02012026.csv\")\n",
        "PRONOUN_CSV_PATH   = Path(\"/content/drive/MyDrive/Colab_Datasets/VV/Wiktionary_Thai_pronoun_02012026.csv\")\n",
        "\n",
        "SPLIT = \"train\"\n",
        "TEXT_FIELD = \"body\"\n",
        "MAX_ROWS = -1\n",
        "CHUNK_WRITE = 200_000\n",
        "SKIP_IF_OUTPUT_EXISTS = True\n",
        "\n",
        "ENABLE = {\n",
        "    \"verb-verb\": True,\n",
        "    \"noun-verb\": True,\n",
        "    \"verb-noun\": True,\n",
        "    \"verb-(noun)-verb\": True,\n",
        "    \"verb-(verb)-verb\": True,\n",
        "    \"adverb-verb\": True,\n",
        "    \"verb-adverb\": True,\n",
        "    \"verb-(adverb)-verb\": True,\n",
        "    \"adjective-verb\": True,\n",
        "    \"verb-adjective\": True,\n",
        "    \"verb-(adjective)-verb\": True,\n",
        "    \"pronoun-verb\": True,\n",
        "    \"verb-pronoun\": True,\n",
        "    \"verb-(pronoun)-verb\": True,\n",
        "}\n",
        "\n",
        "ALLOW_OVERLAP_MIDDLE = {\n",
        "    \"noun\": True,\n",
        "    \"adverb\": True,\n",
        "    \"adjective\": True,\n",
        "    \"pronoun\": True,\n",
        "}\n",
        "\n",
        "OUT = {\n",
        "    \"verb-verb\": OUT_DIR / \"Thaisum_verb-verb_pairs.csv\",\n",
        "\n",
        "    \"noun-verb\": OUT_DIR / \"Thaisum_noun-verb_pairs.csv\",\n",
        "    \"verb-noun\": OUT_DIR / \"Thaisum_verb-noun_pairs.csv\",\n",
        "    \"verb-(noun)-verb\": OUT_DIR / \"Thaisum_verb-(noun)-verb_pairs.csv\",\n",
        "\n",
        "    \"verb-(verb)-verb\": OUT_DIR / \"Thaisum_verb-(verb)-verb_pairs.csv\",\n",
        "\n",
        "    \"adverb-verb\": OUT_DIR / \"Thaisum_adverb-verb_pairs.csv\",\n",
        "    \"verb-adverb\": OUT_DIR / \"Thaisum_verb-adverb_pairs.csv\",\n",
        "    \"verb-(adverb)-verb\": OUT_DIR / \"Thaisum_verb-(adverb)-verb_pairs.csv\",\n",
        "\n",
        "    \"adjective-verb\": OUT_DIR / \"Thaisum_adjective-verb_pairs.csv\",\n",
        "    \"verb-adjective\": OUT_DIR / \"Thaisum_verb-adjective_pairs.csv\",\n",
        "    \"verb-(adjective)-verb\": OUT_DIR / \"Thaisum_verb-(adjective)-verb_pairs.csv\",\n",
        "\n",
        "    \"pronoun-verb\": OUT_DIR / \"Thaisum_pronoun-verb_pairs.csv\",\n",
        "    \"verb-pronoun\": OUT_DIR / \"Thaisum_verb-pronoun_pairs.csv\",\n",
        "    \"verb-(pronoun)-verb\": OUT_DIR / \"Thaisum_verb-(pronoun)-verb_pairs.csv\",\n",
        "}\n",
        "\n",
        "if SKIP_IF_OUTPUT_EXISTS:\n",
        "    for k, path in OUT.items():\n",
        "        if ENABLE.get(k, False) and path.exists():\n",
        "            print(f\"[SKIP] {k} enabled but output exists: {path}\")\n",
        "            ENABLE[k] = False\n",
        "\n",
        "print(\"\\nEnabled jobs:\")\n",
        "for k, v in ENABLE.items():\n",
        "    if v:\n",
        "        print(\"  ✅\", k)\n",
        "\n",
        "THAI_RE = re.compile(r\"^[\\u0E00-\\u0E7F ]+$\")\n",
        "\n",
        "def load_wordlist_onecol(csv_path: Path, label: str):\n",
        "    wdf = pd.read_csv(csv_path, header=None, names=[label], encoding=\"utf-8-sig\")\n",
        "    series = (\n",
        "        wdf[label]\n",
        "        .astype(str)\n",
        "        .str.replace(\"\\ufeff\", \"\", regex=False)\n",
        "        .str.strip()\n",
        "        .replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan})\n",
        "        .dropna()\n",
        "    )\n",
        "    words = [w for w in series.unique().tolist() if THAI_RE.match(w)]\n",
        "    if not words:\n",
        "        raise ValueError(f\"No usable Thai {label}s found in {csv_path}\")\n",
        "    return words\n",
        "\n",
        "def build_automaton(words):\n",
        "    A = ahocorasick.Automaton()\n",
        "    for i, w in enumerate(words):\n",
        "        A.add_word(w, i)\n",
        "    A.make_automaton()\n",
        "    lens = np.array([len(w) for w in words], dtype=np.int32)\n",
        "    return A, lens\n",
        "\n",
        "data_files = {\n",
        "    \"train\": sorted(str(p) for p in DATA_DIR.glob(\"train-*.parquet\")),\n",
        "    \"validation\": sorted(str(p) for p in DATA_DIR.glob(\"validation-*.parquet\")),\n",
        "    \"test\": sorted(str(p) for p in DATA_DIR.glob(\"test-*.parquet\")),\n",
        "}\n",
        "if not data_files[\"train\"]:\n",
        "    raise FileNotFoundError(f\"No train parquet files found in: {DATA_DIR}\")\n",
        "\n",
        "ds = load_dataset(\"parquet\", data_files=data_files)\n",
        "print(ds)\n",
        "\n",
        "VERBS = load_wordlist_onecol(VERB_CSV_PATH, \"verb\")\n",
        "V = len(VERBS)\n",
        "verb_set = set(VERBS)\n",
        "print(\"Loaded verbs:\", V)\n",
        "\n",
        "need_nouns = any(ENABLE.get(k, False) for k in [\"noun-verb\", \"verb-noun\", \"verb-(noun)-verb\"])\n",
        "if need_nouns:\n",
        "    NOUNS = load_wordlist_onecol(NOUN_CSV_PATH, \"noun\")\n",
        "    N = len(NOUNS)\n",
        "    noun_set = set(NOUNS)\n",
        "    OVERLAP_NOUN_VERB = verb_set.intersection(noun_set)\n",
        "    noun_in_overlap = np.array([w in OVERLAP_NOUN_VERB for w in NOUNS], dtype=bool)\n",
        "    print(\"Loaded nouns:\", N, \" overlap(noun∩verb):\", len(OVERLAP_NOUN_VERB))\n",
        "else:\n",
        "    NOUNS = None\n",
        "    N = 0\n",
        "    noun_in_overlap = None\n",
        "\n",
        "POS_LISTS = {}\n",
        "for pos, path in [(\"adverb\", ADVERB_CSV_PATH), (\"adjective\", ADJECTIVE_CSV_PATH), (\"pronoun\", PRONOUN_CSV_PATH)]:\n",
        "    need_pos = any(ENABLE.get(k, False) for k in [f\"{pos}-verb\", f\"verb-{pos}\", f\"verb-({pos})-verb\"])\n",
        "    if need_pos:\n",
        "        words = load_wordlist_onecol(path, pos)\n",
        "        overlap = verb_set.intersection(set(words))\n",
        "        POS_LISTS[pos] = {\n",
        "            \"words\": words,\n",
        "            \"X\": len(words),\n",
        "            \"overlap_set\": overlap,\n",
        "            \"x_in_overlap\": np.array([w in overlap for w in words], dtype=bool)\n",
        "        }\n",
        "        print(f\"Loaded {pos}s:\", len(words), f\" overlap({pos}∩verb):\", len(overlap))\n",
        "\n",
        "A_verb, lens_verb = build_automaton(VERBS)\n",
        "\n",
        "if need_nouns:\n",
        "    A_noun, lens_noun = build_automaton(NOUNS)\n",
        "\n",
        "for pos, spec in POS_LISTS.items():\n",
        "    A_x, lens_x = build_automaton(spec[\"words\"])\n",
        "    spec[\"A\"] = A_x\n",
        "    spec[\"lens\"] = lens_x\n",
        "\n",
        "verbs_arr = np.array(VERBS, dtype=object)\n",
        "\n",
        "if ENABLE.get(\"verb-verb\", False):\n",
        "    TOTAL_VV = V * V\n",
        "    occ_vv = np.zeros(TOTAL_VV, dtype=np.int64)\n",
        "    txt_vv = np.zeros(TOTAL_VV, dtype=np.int32)\n",
        "\n",
        "if ENABLE.get(\"noun-verb\", False):\n",
        "    TOTAL_NV = N * V\n",
        "    occ_nv = np.zeros(TOTAL_NV, dtype=np.int64)\n",
        "    txt_nv = np.zeros(TOTAL_NV, dtype=np.int32)\n",
        "\n",
        "if ENABLE.get(\"verb-noun\", False):\n",
        "    TOTAL_VN = V * N\n",
        "    occ_vn = np.zeros(TOTAL_VN, dtype=np.int64)\n",
        "    txt_vn = np.zeros(TOTAL_VN, dtype=np.int32)\n",
        "\n",
        "if ENABLE.get(\"verb-(noun)-verb\", False):\n",
        "    TOTAL_VV = V * V\n",
        "    occ_vnv = np.zeros(TOTAL_VV, dtype=np.int64)\n",
        "    txt_vnv = np.zeros(TOTAL_VV, dtype=np.int32)\n",
        "\n",
        "if ENABLE.get(\"verb-(verb)-verb\", False):\n",
        "    TOTAL_VV = V * V\n",
        "    occ_vvv = np.zeros(TOTAL_VV, dtype=np.int64)\n",
        "    txt_vvv = np.zeros(TOTAL_VV, dtype=np.int32)\n",
        "\n",
        "for pos, spec in POS_LISTS.items():\n",
        "    X = spec[\"X\"]\n",
        "    if ENABLE.get(f\"{pos}-verb\", False):\n",
        "        spec[\"occ_xv\"] = np.zeros(X * V, dtype=np.int64)\n",
        "        spec[\"txt_xv\"] = np.zeros(X * V, dtype=np.int32)\n",
        "    if ENABLE.get(f\"verb-{pos}\", False):\n",
        "        spec[\"occ_vx\"] = np.zeros(V * X, dtype=np.int64)\n",
        "        spec[\"txt_vx\"] = np.zeros(V * X, dtype=np.int32)\n",
        "    if ENABLE.get(f\"verb-({pos})-verb\", False):\n",
        "        spec[\"occ_vxv\"] = np.zeros(V * V, dtype=np.int64)\n",
        "        spec[\"txt_vxv\"] = np.zeros(V * V, dtype=np.int32)\n",
        "\n",
        "any_enabled = any(ENABLE.values())\n",
        "if not any_enabled:\n",
        "    print(\"\\nNothing enabled. Exiting without scanning.\")\n",
        "    raise SystemExit\n",
        "\n",
        "data = ds[SPLIT]\n",
        "if MAX_ROWS is None or MAX_ROWS == -1:\n",
        "    iterable = data\n",
        "    total_for_tqdm = len(data)\n",
        "else:\n",
        "    n_rows = min(int(MAX_ROWS), len(data))\n",
        "    iterable = data.select(range(n_rows))\n",
        "    total_for_tqdm = n_rows\n",
        "\n",
        "for row in tqdm(iterable, total=total_for_tqdm, desc=f\"Scanning {SPLIT}\"):\n",
        "    text = row.get(TEXT_FIELD, \"\") or \"\"\n",
        "\n",
        "    v_starts = defaultdict(list)\n",
        "    v_ends   = defaultdict(list)\n",
        "    v_starts_with_end = defaultdict(list)\n",
        "    for end_pos, vid in A_verb.iter(text):\n",
        "        start_pos = end_pos - lens_verb[vid] + 1\n",
        "        v_starts[start_pos].append(vid)\n",
        "        v_ends[end_pos].append(vid)\n",
        "        v_starts_with_end[start_pos].append((vid, end_pos))\n",
        "\n",
        "    if need_nouns:\n",
        "        n_starts = defaultdict(list)\n",
        "        n_ends   = defaultdict(list)\n",
        "        n_starts_with_end = defaultdict(list)\n",
        "        for end_pos, nid in A_noun.iter(text):\n",
        "            start_pos = end_pos - lens_noun[nid] + 1\n",
        "            n_starts[start_pos].append(nid)\n",
        "            n_ends[end_pos].append(nid)\n",
        "            n_starts_with_end[start_pos].append((nid, end_pos))\n",
        "\n",
        "    if ENABLE.get(\"verb-verb\", False):\n",
        "        seen = set()\n",
        "        for e, left_vs in v_ends.items():\n",
        "            right_vs = v_starts.get(e + 1)\n",
        "            if not right_vs:\n",
        "                continue\n",
        "            for v1 in left_vs:\n",
        "                base = v1 * V\n",
        "                for v2 in right_vs:\n",
        "                    code = base + v2\n",
        "                    occ_vv[code] += 1\n",
        "                    seen.add(code)\n",
        "        for code in seen:\n",
        "            txt_vv[code] += 1\n",
        "\n",
        "    if ENABLE.get(\"verb-(verb)-verb\", False):\n",
        "        seen = set()\n",
        "        for e, left_vs in v_ends.items():\n",
        "            mids = v_starts_with_end.get(e + 1)\n",
        "            if not mids:\n",
        "                continue\n",
        "            for v1 in left_vs:\n",
        "                base = v1 * V\n",
        "                for (vmid, mid_end) in mids:\n",
        "                    right_vs = v_starts.get(mid_end + 1)\n",
        "                    if not right_vs:\n",
        "                        continue\n",
        "                    for v2 in right_vs:\n",
        "                        code = base + v2\n",
        "                        occ_vvv[code] += 1\n",
        "                        seen.add(code)\n",
        "        for code in seen:\n",
        "            txt_vvv[code] += 1\n",
        "\n",
        "    if ENABLE.get(\"noun-verb\", False):\n",
        "        seen = set()\n",
        "        for e, left_ns in n_ends.items():\n",
        "            right_vs = v_starts.get(e + 1)\n",
        "            if not right_vs:\n",
        "                continue\n",
        "            for nid in left_ns:\n",
        "                if noun_in_overlap[nid]:\n",
        "                    continue\n",
        "                base = nid * V\n",
        "                for vid in right_vs:\n",
        "                    code = base + vid\n",
        "                    occ_nv[code] += 1\n",
        "                    seen.add(code)\n",
        "        for code in seen:\n",
        "            txt_nv[code] += 1\n",
        "\n",
        "    if ENABLE.get(\"verb-noun\", False):\n",
        "        seen = set()\n",
        "        for e, left_vs in v_ends.items():\n",
        "            right_ns = n_starts.get(e + 1)\n",
        "            if not right_ns:\n",
        "                continue\n",
        "            for vid in left_vs:\n",
        "                base = vid * N\n",
        "                for nid in right_ns:\n",
        "                    if noun_in_overlap[nid]:\n",
        "                        continue\n",
        "                    code = base + nid\n",
        "                    occ_vn[code] += 1\n",
        "                    seen.add(code)\n",
        "        for code in seen:\n",
        "            txt_vn[code] += 1\n",
        "\n",
        "    if ENABLE.get(\"verb-(noun)-verb\", False):\n",
        "        allow_mid = ALLOW_OVERLAP_MIDDLE.get(\"noun\", True)\n",
        "        seen = set()\n",
        "        for e, left_vs in v_ends.items():\n",
        "            mids = n_starts_with_end.get(e + 1)\n",
        "            if not mids:\n",
        "                continue\n",
        "            for v1 in left_vs:\n",
        "                base = v1 * V\n",
        "                for (nid, n_end) in mids:\n",
        "                    if (not allow_mid) and noun_in_overlap[nid]:\n",
        "                        continue\n",
        "                    right_vs = v_starts.get(n_end + 1)\n",
        "                    if not right_vs:\n",
        "                        continue\n",
        "                    for v2 in right_vs:\n",
        "                        code = base + v2\n",
        "                        occ_vnv[code] += 1\n",
        "                        seen.add(code)\n",
        "        for code in seen:\n",
        "            txt_vnv[code] += 1\n",
        "\n",
        "    for pos, spec in POS_LISTS.items():\n",
        "        A_x = spec[\"A\"]\n",
        "        lens_x = spec[\"lens\"]\n",
        "        X = spec[\"X\"]\n",
        "        x_in_overlap = spec[\"x_in_overlap\"]\n",
        "        allow_mid = ALLOW_OVERLAP_MIDDLE.get(pos, True)\n",
        "\n",
        "        x_starts = defaultdict(list)\n",
        "        x_ends   = defaultdict(list)\n",
        "        x_starts_with_end = defaultdict(list)\n",
        "        for end_pos, xid in A_x.iter(text):\n",
        "            start_pos = end_pos - lens_x[xid] + 1\n",
        "            x_starts[start_pos].append(xid)\n",
        "            x_ends[end_pos].append(xid)\n",
        "            x_starts_with_end[start_pos].append((xid, end_pos))\n",
        "\n",
        "        if ENABLE.get(f\"{pos}-verb\", False):\n",
        "            seen = set()\n",
        "            for e, left_xs in x_ends.items():\n",
        "                right_vs = v_starts.get(e + 1)\n",
        "                if not right_vs:\n",
        "                    continue\n",
        "                for xid in left_xs:\n",
        "                    if x_in_overlap[xid]:\n",
        "                        continue\n",
        "                    base = xid * V\n",
        "                    for vid in right_vs:\n",
        "                        code = base + vid\n",
        "                        spec[\"occ_xv\"][code] += 1\n",
        "                        seen.add(code)\n",
        "            for code in seen:\n",
        "                spec[\"txt_xv\"][code] += 1\n",
        "\n",
        "        if ENABLE.get(f\"verb-{pos}\", False):\n",
        "            seen = set()\n",
        "            for e, left_vs in v_ends.items():\n",
        "                right_xs = x_starts.get(e + 1)\n",
        "                if not right_xs:\n",
        "                    continue\n",
        "                for vid in left_vs:\n",
        "                    base = vid * X\n",
        "                    for xid in right_xs:\n",
        "                        if x_in_overlap[xid]:\n",
        "                            continue\n",
        "                        code = base + xid\n",
        "                        spec[\"occ_vx\"][code] += 1\n",
        "                        seen.add(code)\n",
        "            for code in seen:\n",
        "                spec[\"txt_vx\"][code] += 1\n",
        "\n",
        "        if ENABLE.get(f\"verb-({pos})-verb\", False):\n",
        "            seen = set()\n",
        "            for e, left_vs in v_ends.items():\n",
        "                mids = x_starts_with_end.get(e + 1)\n",
        "                if not mids:\n",
        "                    continue\n",
        "                for v1 in left_vs:\n",
        "                    base = v1 * V\n",
        "                    for (xid, x_end) in mids:\n",
        "                        if (not allow_mid) and x_in_overlap[xid]:\n",
        "                            continue\n",
        "                        right_vs = v_starts.get(x_end + 1)\n",
        "                        if not right_vs:\n",
        "                            continue\n",
        "                        for v2 in right_vs:\n",
        "                            code = base + v2\n",
        "                            spec[\"occ_vxv\"][code] += 1\n",
        "                            seen.add(code)\n",
        "            for code in seen:\n",
        "                spec[\"txt_vxv\"][code] += 1\n",
        "\n",
        "print(\"Done counting.\")\n",
        "\n",
        "def write_adj_left_right(left_words, right_words, occ, txt, out_path, left_name, right_name,\n",
        "                         skip_left_mask=None, total_chunk=CHUNK_WRITE):\n",
        "    L = len(left_words)\n",
        "    R = len(right_words)\n",
        "    left_arr = np.array(left_words, dtype=object)\n",
        "    right_arr = np.array(right_words, dtype=object)\n",
        "    TOTAL = L * R\n",
        "\n",
        "    wrote_header = False\n",
        "    for start in tqdm(range(0, TOTAL, total_chunk), desc=f\"Writing {left_name}-{right_name} CSV\"):\n",
        "        end = min(start + total_chunk, TOTAL)\n",
        "        idx = np.arange(start, end, dtype=np.int64)\n",
        "        l_ids = (idx // R).astype(np.int64)\n",
        "        r_ids = (idx %  R).astype(np.int64)\n",
        "\n",
        "        keep = np.ones_like(l_ids, dtype=bool)\n",
        "        if skip_left_mask is not None:\n",
        "            keep = ~skip_left_mask[l_ids]\n",
        "        if not np.any(keep):\n",
        "            continue\n",
        "\n",
        "        l_ids_k = l_ids[keep]\n",
        "        r_ids_k = r_ids[keep]\n",
        "        codes   = (l_ids_k * R + r_ids_k).astype(np.int64)\n",
        "\n",
        "        left_col = left_arr[l_ids_k]\n",
        "        right_col = right_arr[r_ids_k]\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            left_name: left_col,\n",
        "            right_name: right_col,\n",
        "            \"needle\": (left_col + right_col),\n",
        "            \"total_occurrences\": occ[codes],\n",
        "            \"texts_with_match\": txt[codes],\n",
        "        })\n",
        "\n",
        "        df.to_csv(\n",
        "            out_path,\n",
        "            mode=\"w\" if not wrote_header else \"a\",\n",
        "            header=(not wrote_header),\n",
        "            index=False,\n",
        "            encoding=\"utf-8-sig\"\n",
        "        )\n",
        "        wrote_header = True\n",
        "\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "def write_vv_like(occ, txt, out_path, marker):\n",
        "    TOTAL = V * V\n",
        "    for start in tqdm(range(0, TOTAL, CHUNK_WRITE), desc=f\"Writing {out_path.name}\"):\n",
        "        end = min(start + CHUNK_WRITE, TOTAL)\n",
        "        idx = np.arange(start, end, dtype=np.int64)\n",
        "        p_ids = (idx // V).astype(np.int64)\n",
        "        s_ids = (idx %  V).astype(np.int64)\n",
        "\n",
        "        pref = verbs_arr[p_ids]\n",
        "        suff = verbs_arr[s_ids]\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"prefix\": pref,\n",
        "            \"suffix\": suff,\n",
        "            \"needle\": (pref + marker + suff),\n",
        "            \"total_occurrences\": occ[start:end],\n",
        "            \"texts_with_match\": txt[start:end],\n",
        "        })\n",
        "\n",
        "        df.to_csv(\n",
        "            out_path,\n",
        "            mode=\"w\" if start == 0 else \"a\",\n",
        "            header=(start == 0),\n",
        "            index=False,\n",
        "            encoding=\"utf-8-sig\"\n",
        "        )\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "if ENABLE.get(\"verb-verb\", False):\n",
        "    write_vv_like(occ_vv, txt_vv, OUT[\"verb-verb\"], marker=\"\")\n",
        "\n",
        "if ENABLE.get(\"verb-(verb)-verb\", False):\n",
        "    write_vv_like(occ_vvv, txt_vvv, OUT[\"verb-(verb)-verb\"], marker=\"<V>\")\n",
        "\n",
        "if ENABLE.get(\"noun-verb\", False):\n",
        "    write_adj_left_right(\n",
        "        left_words=NOUNS, right_words=VERBS,\n",
        "        occ=occ_nv, txt=txt_nv,\n",
        "        out_path=OUT[\"noun-verb\"],\n",
        "        left_name=\"noun\", right_name=\"verb\",\n",
        "        skip_left_mask=noun_in_overlap\n",
        "    )\n",
        "\n",
        "if ENABLE.get(\"verb-noun\", False):\n",
        "    out_path = OUT[\"verb-noun\"]\n",
        "    wrote_header = False\n",
        "    for start in tqdm(range(0, V * N, CHUNK_WRITE), desc=\"Writing verb-noun CSV\"):\n",
        "        end = min(start + CHUNK_WRITE, V * N)\n",
        "        idx = np.arange(start, end, dtype=np.int64)\n",
        "\n",
        "        v_ids = (idx // N).astype(np.int64)\n",
        "        n_ids = (idx %  N).astype(np.int64)\n",
        "\n",
        "        keep = ~noun_in_overlap[n_ids]\n",
        "        if not np.any(keep):\n",
        "            continue\n",
        "\n",
        "        v_ids_k = v_ids[keep]\n",
        "        n_ids_k = n_ids[keep]\n",
        "        codes   = (v_ids_k * N + n_ids_k).astype(np.int64)\n",
        "\n",
        "        verb_col = verbs_arr[v_ids_k]\n",
        "        noun_col = np.array(NOUNS, dtype=object)[n_ids_k]\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"verb\": verb_col,\n",
        "            \"noun\": noun_col,\n",
        "            \"needle\": (verb_col + noun_col),\n",
        "            \"total_occurrences\": occ_vn[codes],\n",
        "            \"texts_with_match\": txt_vn[codes],\n",
        "        })\n",
        "\n",
        "        df.to_csv(\n",
        "            out_path,\n",
        "            mode=\"w\" if not wrote_header else \"a\",\n",
        "            header=(not wrote_header),\n",
        "            index=False,\n",
        "            encoding=\"utf-8-sig\"\n",
        "        )\n",
        "        wrote_header = True\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "if ENABLE.get(\"verb-(noun)-verb\", False):\n",
        "    write_vv_like(occ_vnv, txt_vnv, OUT[\"verb-(noun)-verb\"], marker=\"<N>\")\n",
        "\n",
        "for pos, spec in POS_LISTS.items():\n",
        "    if ENABLE.get(f\"{pos}-verb\", False):\n",
        "        write_adj_left_right(\n",
        "            left_words=spec[\"words\"], right_words=VERBS,\n",
        "            occ=spec[\"occ_xv\"], txt=spec[\"txt_xv\"],\n",
        "            out_path=OUT[f\"{pos}-verb\"],\n",
        "            left_name=pos, right_name=\"verb\",\n",
        "            skip_left_mask=spec[\"x_in_overlap\"]\n",
        "        )\n",
        "\n",
        "    if ENABLE.get(f\"verb-{pos}\", False):\n",
        "        out_path = OUT[f\"verb-{pos}\"]\n",
        "        X = spec[\"X\"]\n",
        "        x_in_overlap = spec[\"x_in_overlap\"]\n",
        "        pos_arr = np.array(spec[\"words\"], dtype=object)\n",
        "\n",
        "        wrote_header = False\n",
        "        for start in tqdm(range(0, V * X, CHUNK_WRITE), desc=f\"Writing verb-{pos} CSV\"):\n",
        "            end = min(start + CHUNK_WRITE, V * X)\n",
        "            idx = np.arange(start, end, dtype=np.int64)\n",
        "\n",
        "            v_ids = (idx // X).astype(np.int64)\n",
        "            x_ids = (idx %  X).astype(np.int64)\n",
        "\n",
        "            keep = ~x_in_overlap[x_ids]\n",
        "            if not np.any(keep):\n",
        "                continue\n",
        "\n",
        "            v_ids_k = v_ids[keep]\n",
        "            x_ids_k = x_ids[keep]\n",
        "            codes   = (v_ids_k * X + x_ids_k).astype(np.int64)\n",
        "\n",
        "            verb_col = verbs_arr[v_ids_k]\n",
        "            pos_col  = pos_arr[x_ids_k]\n",
        "\n",
        "            df = pd.DataFrame({\n",
        "                \"verb\": verb_col,\n",
        "                pos: pos_col,\n",
        "                \"needle\": (verb_col + pos_col),\n",
        "                \"total_occurrences\": spec[\"occ_vx\"][codes],\n",
        "                \"texts_with_match\": spec[\"txt_vx\"][codes],\n",
        "            })\n",
        "\n",
        "            df.to_csv(\n",
        "                out_path,\n",
        "                mode=\"w\" if not wrote_header else \"a\",\n",
        "                header=(not wrote_header),\n",
        "                index=False,\n",
        "                encoding=\"utf-8-sig\"\n",
        "            )\n",
        "            wrote_header = True\n",
        "        print(\"Saved:\", out_path)\n",
        "\n",
        "    if ENABLE.get(f\"verb-({pos})-verb\", False):\n",
        "        marker = f\"<{pos[:1].upper()}>\"\n",
        "        write_vv_like(spec[\"occ_vxv\"], spec[\"txt_vxv\"], OUT[f\"verb-({pos})-verb\"], marker=marker)\n",
        "\n",
        "print(\"\\nALL DONE ✅\")"
      ],
      "metadata": {
        "id": "koHZNhPyZWDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cluster creator**"
      ],
      "metadata": {
        "id": "1R6GvYXJYs0H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7u9kWFLYC-l"
      },
      "outputs": [],
      "source": [
        "!pip -q install pandas numpy scipy scikit-learn tqdm pynndescent matplotlib ipywidgets\n",
        "\n",
        "import os, re, time, json, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.manifold import SpectralEmbedding\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from pynndescent import NNDescent\n",
        "\n",
        "_UI_AVAILABLE = True\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except Exception:\n",
        "    _UI_AVAILABLE = False\n",
        "\n",
        "class ProgressUI:\n",
        "    def __init__(self, enabled: bool = True, show_log: bool = False):\n",
        "        self.enabled = bool(enabled) and _UI_AVAILABLE\n",
        "        self.show_log = bool(show_log)\n",
        "        self._overall_total = 1\n",
        "        self._overall_value = 0\n",
        "        self._run_total = 1\n",
        "        self._run_value = 0\n",
        "\n",
        "        if not self.enabled:\n",
        "            self.box = None\n",
        "            return\n",
        "\n",
        "        self.status = widgets.HTML(value=\"<b>Status:</b> idle\")\n",
        "        self.overall = widgets.IntProgress(value=0, min=0, max=1, description=\"Overall\", bar_style=\"\")\n",
        "        self.runbar = widgets.IntProgress(value=0, min=0, max=1, description=\"Run\", bar_style=\"\")\n",
        "        self._log = widgets.Output(layout={\"border\": \"1px solid #ddd\", \"max_height\": \"160px\", \"overflow_y\": \"auto\"}) if self.show_log else None\n",
        "\n",
        "        items = [self.status, self.overall, self.runbar]\n",
        "        if self._log is not None:\n",
        "            items.append(self._log)\n",
        "\n",
        "        self.box = widgets.VBox(items)\n",
        "        display(self.box)\n",
        "\n",
        "    def set_status(self, text: str):\n",
        "        if self.enabled:\n",
        "            self.status.value = f\"<b>Status:</b> {text}\"\n",
        "        else:\n",
        "            print(text)\n",
        "\n",
        "    def log(self, text: str):\n",
        "        if not self.enabled or self._log is None:\n",
        "            return\n",
        "        with self._log:\n",
        "            print(text)\n",
        "\n",
        "    def start_overall(self, total_steps: int, text: str = \"starting...\"):\n",
        "        self._overall_total = int(max(1, total_steps))\n",
        "        self._overall_value = 0\n",
        "        if self.enabled:\n",
        "            self.overall.max = self._overall_total\n",
        "            self.overall.value = 0\n",
        "            self.overall.bar_style = \"\"\n",
        "        self.set_status(text)\n",
        "\n",
        "    def step_overall(self, n: int = 1, text: str | None = None):\n",
        "        self._overall_value = min(self._overall_total, self._overall_value + int(n))\n",
        "        if self.enabled:\n",
        "            self.overall.value = self._overall_value\n",
        "        if text is not None:\n",
        "            self.set_status(text)\n",
        "\n",
        "    def start_run(self, total_steps: int, text: str = \"run starting...\"):\n",
        "        self._run_total = int(max(1, total_steps))\n",
        "        self._run_value = 0\n",
        "        if self.enabled:\n",
        "            self.runbar.max = self._run_total\n",
        "            self.runbar.value = 0\n",
        "            self.runbar.bar_style = \"\"\n",
        "        self.set_status(text)\n",
        "\n",
        "    def step_run(self, n: int = 1, text: str | None = None):\n",
        "        self._run_value = min(self._run_total, self._run_value + int(n))\n",
        "        if self.enabled:\n",
        "            self.runbar.value = self._run_value\n",
        "        if text is not None:\n",
        "            self.set_status(text)\n",
        "\n",
        "    def mark_done(self, text: str = \"DONE\"):\n",
        "        if self.enabled:\n",
        "            self.overall.bar_style = \"success\"\n",
        "            self.runbar.bar_style = \"success\"\n",
        "        self.set_status(text)\n",
        "\n",
        "_MAX_COMPONENT = 140\n",
        "\n",
        "def _safe_name(s: str, max_len: int = _MAX_COMPONENT) -> str:\n",
        "    s = str(s).strip()\n",
        "    s = re.sub(r\"\\s+\", \"_\", s)\n",
        "    s = re.sub(r\"[^A-Za-z0-9._\\-]+\", \"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s)\n",
        "    if not s:\n",
        "        s = \"x\"\n",
        "    return s[:max_len] if len(s) > max_len else s\n",
        "\n",
        "def _now_str():\n",
        "    return time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def _ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def _atomic_replace(tmp_path: Path, final_path: Path):\n",
        "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.replace(str(tmp_path), str(final_path))\n",
        "\n",
        "def _tmp_path(final_path: Path, ext: str | None = None) -> Path:\n",
        "    \"\"\"\n",
        "    Make a short temp filename in the same directory, to avoid Drive filename limits.\n",
        "    \"\"\"\n",
        "    ext = ext if ext is not None else final_path.suffix\n",
        "    ts = int(time.time() * 1_000_000)\n",
        "    # leading dot keeps it hidden; name stays short\n",
        "    return final_path.with_name(f\".tmp_{ts}_{os.getpid()}{ext}\")\n",
        "\n",
        "def _atomic_write_text(path: Path, text: str):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = _tmp_path(path, \".tmp\")\n",
        "    tmp.write_text(text, encoding=\"utf-8\")\n",
        "    _atomic_replace(tmp, path)\n",
        "\n",
        "def _atomic_write_json(path: Path, obj: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = _tmp_path(path, \".tmp\")\n",
        "    tmp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    _atomic_replace(tmp, path)\n",
        "\n",
        "def _read_json(path: Path) -> dict | None:\n",
        "    try:\n",
        "        if path.exists():\n",
        "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def _atomic_write_df_csv(df: pd.DataFrame, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = _tmp_path(path, \".csv\")\n",
        "    df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
        "    _atomic_replace(tmp, path)\n",
        "\n",
        "def _safe_read_csv(path: Path) -> pd.DataFrame:\n",
        "    try:\n",
        "        if path.exists():\n",
        "            return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def _atomic_save_npz_array(path: Path, **arrays):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = _tmp_path(path, \".npz\")\n",
        "    np.savez_compressed(tmp, **arrays)\n",
        "    _atomic_replace(tmp, path)\n",
        "\n",
        "def _atomic_save_sparse_npz(path: Path, mat: sp.spmatrix):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    tmp = _tmp_path(path, \".npz\")\n",
        "    sp.save_npz(tmp, mat)\n",
        "    _atomic_replace(tmp, path)\n",
        "\n",
        "def _make_seeds(base_seed: int, n: int):\n",
        "    rng = np.random.RandomState(int(base_seed))\n",
        "    return [int(x) for x in rng.randint(0, 2**31 - 1, size=int(n), dtype=np.int64).tolist()]\n",
        "\n",
        "def _fingerprint(cfg: dict, *, features: list[str], knn: int, k: int, emb_dims: int) -> str:\n",
        "    relevant = {\n",
        "        \"features\": list(features),\n",
        "        \"knn\": int(knn),\n",
        "        \"k\": int(k),\n",
        "        \"emb_dims\": int(emb_dims),\n",
        "        \"data\": dict(cfg[\"DATA\"]),\n",
        "        \"graph\": dict(cfg[\"GRAPH\"]),\n",
        "        \"seeds\": dict(cfg[\"SEEDS\"]),\n",
        "        \"mncut\": dict(cfg[\"MNCUT\"]),\n",
        "    }\n",
        "    blob = json.dumps(relevant, sort_keys=True, ensure_ascii=False).encode(\"utf-8\")\n",
        "    return hashlib.md5(blob).hexdigest()\n",
        "\n",
        "def _feature_tag(feature_names: list[str], *, all_feature_names: list[str] | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Short, stable tag for filenames.\n",
        "    - If all features selected -> ALL\n",
        "    - Else -> F{n}_{hash8}\n",
        "    \"\"\"\n",
        "    if all_feature_names is not None and set(feature_names) == set(all_feature_names):\n",
        "        return \"ALL\"\n",
        "    s = \"||\".join(sorted(feature_names))\n",
        "    h = hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:8]\n",
        "    return f\"F{len(feature_names)}_{h}\"\n",
        "\n",
        "def load_verbs(verb_list_csv: Path):\n",
        "    verbs = (\n",
        "        pd.read_csv(verb_list_csv, header=None, encoding=\"utf-8-sig\")[0]\n",
        "        .astype(str).str.replace(\"\\ufeff\", \"\", regex=False).str.strip()\n",
        "    )\n",
        "    verbs = verbs[verbs != \"\"].tolist()\n",
        "    vid = {v: i for i, v in enumerate(verbs)}\n",
        "    return verbs, vid\n",
        "\n",
        "def read_verb_context_matrix(path: Path, *, vid: dict, V: int,\n",
        "                            verb_col: str, ctx_col: str,\n",
        "                            use_col: str, min_count: float, chunk: int,\n",
        "                            weight_transform: str):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    ctx2id = {}\n",
        "    rows_parts, cols_parts, data_parts = [], [], []\n",
        "    usecols = [verb_col, ctx_col, use_col]\n",
        "\n",
        "    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding=\"utf-8-sig\"),\n",
        "                   desc=f\"Reading {path.name}\"):\n",
        "        df = df.dropna()\n",
        "        if df.empty:\n",
        "            continue\n",
        "        df = df[df[use_col] >= min_count]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        v = df[verb_col].astype(str).map(vid)\n",
        "        m = v.notna()\n",
        "        if not m.any():\n",
        "            continue\n",
        "\n",
        "        v = v[m].astype(np.int32).to_numpy()\n",
        "        ctx_words = df.loc[m, ctx_col].astype(str).to_numpy()\n",
        "        w = df.loc[m, use_col].to_numpy(np.float32)\n",
        "\n",
        "        if weight_transform == \"raw\":\n",
        "            pass\n",
        "        elif weight_transform == \"log1p\":\n",
        "            w = np.log1p(w).astype(np.float32)\n",
        "        elif weight_transform == \"sqrt\":\n",
        "            w = np.sqrt(w).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown weight_transform: {weight_transform}\")\n",
        "\n",
        "        c = np.empty_like(v, dtype=np.int32)\n",
        "        for i, cw in enumerate(ctx_words):\n",
        "            j = ctx2id.get(cw)\n",
        "            if j is None:\n",
        "                j = len(ctx2id)\n",
        "                ctx2id[cw] = j\n",
        "            c[i] = j\n",
        "\n",
        "        rows_parts.append(v); cols_parts.append(c); data_parts.append(w)\n",
        "\n",
        "    if not rows_parts:\n",
        "        raise RuntimeError(f\"No usable edges in {path.name}.\")\n",
        "\n",
        "    rows = np.concatenate(rows_parts)\n",
        "    cols = np.concatenate(cols_parts)\n",
        "    data = np.concatenate(data_parts)\n",
        "\n",
        "    X = sp.coo_matrix((data, (rows, cols)), shape=(V, len(ctx2id)), dtype=np.float32).tocsr()\n",
        "    X.sum_duplicates()\n",
        "    return X\n",
        "\n",
        "def read_verb_verb_matrix(path: Path, *, vid: dict, V: int,\n",
        "                          prefix_col: str, suffix_col: str,\n",
        "                          use_col: str, min_count: float, chunk: int,\n",
        "                          weight_transform: str):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    rows_parts, cols_parts, data_parts = [], [], []\n",
        "    usecols = [prefix_col, suffix_col, use_col]\n",
        "\n",
        "    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding=\"utf-8-sig\"),\n",
        "                   desc=f\"Reading {path.name}\"):\n",
        "        df = df.dropna()\n",
        "        if df.empty:\n",
        "            continue\n",
        "        df = df[df[use_col] >= min_count]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        r = df[prefix_col].astype(str).map(vid)\n",
        "        c = df[suffix_col].astype(str).map(vid)\n",
        "        m = r.notna() & c.notna()\n",
        "        if not m.any():\n",
        "            continue\n",
        "\n",
        "        r = r[m].astype(np.int32).to_numpy()\n",
        "        c = c[m].astype(np.int32).to_numpy()\n",
        "        w = df.loc[m, use_col].to_numpy(np.float32)\n",
        "\n",
        "        if weight_transform == \"raw\":\n",
        "            pass\n",
        "        elif weight_transform == \"log1p\":\n",
        "            w = np.log1p(w).astype(np.float32)\n",
        "        elif weight_transform == \"sqrt\":\n",
        "            w = np.sqrt(w).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown weight_transform: {weight_transform}\")\n",
        "\n",
        "        rows_parts.append(r); cols_parts.append(c); data_parts.append(w)\n",
        "\n",
        "    if not rows_parts:\n",
        "        raise RuntimeError(f\"No usable edges in {path.name}.\")\n",
        "\n",
        "    rows = np.concatenate(rows_parts)\n",
        "    cols = np.concatenate(cols_parts)\n",
        "    data = np.concatenate(data_parts)\n",
        "\n",
        "    A = sp.coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float32).tocsr()\n",
        "    A.sum_duplicates()\n",
        "    A.setdiag(0)\n",
        "    A.eliminate_zeros()\n",
        "    return A\n",
        "\n",
        "def apply_verbverb_mode(A: sp.csr_matrix, mode: str):\n",
        "    if mode == \"out\":\n",
        "        return A\n",
        "    if mode == \"in\":\n",
        "        return A.T.tocsr()\n",
        "    if mode == \"both\":\n",
        "        return sp.hstack([A, A.T], format=\"csr\")\n",
        "    raise ValueError(\"VERBVERB_MODE must be out/in/both\")\n",
        "\n",
        "def default_feature_specs(base_dir: str):\n",
        "    BASE = Path(base_dir)\n",
        "    return {\n",
        "        \"verb-verb\":             {\"path\": str(BASE / \"Thaisum_verb-verb_pairs.csv\"),             \"kind\": \"verbverb\"},\n",
        "        \"verb-(noun)-verb\":      {\"path\": str(BASE / \"Thaisum_verb-(noun)-verb_pairs.csv\"),      \"kind\": \"verbverb\"},\n",
        "        \"verb-(verb)-verb\":      {\"path\": str(BASE / \"Thaisum_verb-(verb)-verb_pairs.csv\"),      \"kind\": \"verbverb\"},\n",
        "        \"verb-(adjective)-verb\": {\"path\": str(BASE / \"Thaisum_verb-(adjective)-verb_pairs.csv\"), \"kind\": \"verbverb\"},\n",
        "        \"verb-(adverb)-verb\":    {\"path\": str(BASE / \"Thaisum_verb-(adverb)-verb_pairs.csv\"),    \"kind\": \"verbverb\"},\n",
        "        \"verb-(pronoun)-verb\":   {\"path\": str(BASE / \"Thaisum_verb-(pronoun)-verb_pairs.csv\"),   \"kind\": \"verbverb\"},\n",
        "\n",
        "        \"noun-verb\":             {\"path\": str(BASE / \"Thaisum_noun-verb_pairs.csv\"),             \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"noun\"},\n",
        "        \"adjective-verb\":        {\"path\": str(BASE / \"Thaisum_adjective-verb_pairs.csv\"),        \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adjective\"},\n",
        "        \"adverb-verb\":           {\"path\": str(BASE / \"Thaisum_adverb-verb_pairs.csv\"),           \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adverb\"},\n",
        "        \"pronoun-verb\":          {\"path\": str(BASE / \"Thaisum_pronoun-verb_pairs.csv\"),          \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"pronoun\"},\n",
        "\n",
        "        \"verb-noun\":             {\"path\": str(BASE / \"Thaisum_verb-noun_pairs.csv\"),             \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"noun\"},\n",
        "        \"verb-adjective\":        {\"path\": str(BASE / \"Thaisum_verb-adjective_pairs.csv\"),        \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adjective\"},\n",
        "        \"verb-adverb\":           {\"path\": str(BASE / \"Thaisum_verb-adverb_pairs.csv\"),           \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adverb\"},\n",
        "        \"verb-pronoun\":          {\"path\": str(BASE / \"Thaisum_verb-pronoun_pairs.csv\"),          \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"pronoun\"},\n",
        "    }\n",
        "\n",
        "def build_graph_and_embedding(X: sp.csr_matrix, *, cfg, knn: int, emb_dims: int, ui: ProgressUI | None = None):\n",
        "    if ui: ui.step_run(0, \"Finding active verbs...\")\n",
        "    active = X.getnnz(axis=1) > 0\n",
        "    active_ids = np.where(active)[0].astype(np.int32)\n",
        "    X_act = X[active]\n",
        "    n = X_act.shape[0]\n",
        "    if n < int(cfg[\"GRAPH\"][\"MIN_ACTIVE_VERBS\"]):\n",
        "        raise RuntimeError(f\"too_few_active (n={n})\")\n",
        "\n",
        "    if ui: ui.step_run(0, \"Normalizing rows...\")\n",
        "    Xn = normalize(X_act, axis=1)\n",
        "\n",
        "    k_eff = int(min(int(knn), n - 1))\n",
        "    if k_eff < 1:\n",
        "        raise RuntimeError(\"too_few_active_neighbors\")\n",
        "\n",
        "    if ui: ui.step_run(0, f\"Building KNN graph (k={k_eff})...\")\n",
        "    nn = NNDescent(\n",
        "        Xn,\n",
        "        n_neighbors=int(k_eff + 1),\n",
        "        metric=str(cfg[\"GRAPH\"][\"KNN_METRIC\"]),\n",
        "        random_state=int(cfg[\"SEEDS\"][\"EMB_RANDOM_STATE\"]),\n",
        "        n_jobs=int(cfg[\"GRAPH\"][\"N_JOBS\"]),\n",
        "    )\n",
        "    knn_idx, knn_dist = nn.neighbor_graph\n",
        "    knn_sim = 1.0 - knn_dist\n",
        "\n",
        "    I = np.repeat(np.arange(n, dtype=np.int32), k_eff)\n",
        "    J = knn_idx[:, 1:k_eff+1].reshape(-1).astype(np.int32)\n",
        "    S = knn_sim[:, 1:k_eff+1].reshape(-1).astype(np.float32)\n",
        "\n",
        "    pos = S > float(cfg[\"GRAPH\"][\"SIM_MIN\"])\n",
        "    I, J, S = I[pos], J[pos], S[pos]\n",
        "\n",
        "    if ui: ui.step_run(0, \"Building symmetric affinity matrix...\")\n",
        "    W = sp.coo_matrix((S, (I, J)), shape=(n, n), dtype=np.float32).tocsr()\n",
        "    if cfg[\"GRAPH\"][\"MAKE_SYMMETRIC\"]:\n",
        "        W = (W + W.T).tocsr()\n",
        "    W.sum_duplicates()\n",
        "\n",
        "    W_diag0 = W.copy().tocsr()\n",
        "    W_diag0.setdiag(0)\n",
        "    W_diag0.eliminate_zeros()\n",
        "\n",
        "    diag_eps = float(cfg[\"GRAPH\"][\"DIAG_EPS\"])\n",
        "    W_emb = (W + sp.eye(n, dtype=np.float32) * diag_eps).tocsr()\n",
        "\n",
        "    emb_dim_used = int(min(int(emb_dims), n - 2))\n",
        "    if emb_dim_used < 2:\n",
        "        raise RuntimeError(f\"n too small for embedding: n={n}, emb_dim_used={emb_dim_used}\")\n",
        "\n",
        "    if ui: ui.step_run(0, f\"Spectral embedding (dim={emb_dim_used})...\")\n",
        "    Z = SpectralEmbedding(\n",
        "        n_components=emb_dim_used,\n",
        "        affinity=\"precomputed\",\n",
        "        random_state=int(cfg[\"SEEDS\"][\"EMB_RANDOM_STATE\"]),\n",
        "    ).fit_transform(W_emb)\n",
        "\n",
        "    Z = normalize(Z, axis=1)\n",
        "    return active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used\n",
        "\n",
        "def _load_embed_cache(embed_npz: Path, wdiag_npz: Path, expected_fp: str):\n",
        "    if not (embed_npz.exists() and wdiag_npz.exists()):\n",
        "        return None\n",
        "    meta = _read_json(embed_npz.with_suffix(\".meta.json\"))\n",
        "    if not meta or meta.get(\"fingerprint\") != expected_fp:\n",
        "        return None\n",
        "    try:\n",
        "        npz = np.load(embed_npz, allow_pickle=False)\n",
        "        active_ids = npz[\"active_ids\"].astype(np.int32)\n",
        "        Z = npz[\"Z\"].astype(np.float32)\n",
        "        knn_idx = npz[\"knn_idx\"].astype(np.int32)\n",
        "        knn_sim = npz[\"knn_sim\"].astype(np.float32)\n",
        "        emb_dim_used = int(npz[\"emb_dim_used\"][0])\n",
        "        W_diag0 = sp.load_npz(wdiag_npz).tocsr()\n",
        "        return active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _save_embed_cache(embed_npz: Path, wdiag_npz: Path, *, fp: str,\n",
        "                      active_ids, Z, knn_idx, knn_sim, emb_dim_used, W_diag0):\n",
        "    _atomic_save_npz_array(\n",
        "        embed_npz,\n",
        "        active_ids=np.asarray(active_ids, dtype=np.int32),\n",
        "        Z=np.asarray(Z, dtype=np.float32),\n",
        "        knn_idx=np.asarray(knn_idx, dtype=np.int32),\n",
        "        knn_sim=np.asarray(knn_sim, dtype=np.float32),\n",
        "        emb_dim_used=np.asarray([int(emb_dim_used)], dtype=np.int32),\n",
        "    )\n",
        "    _atomic_save_sparse_npz(wdiag_npz, W_diag0)\n",
        "    _atomic_write_json(embed_npz.with_suffix(\".meta.json\"), {\"fingerprint\": fp})\n",
        "\n",
        "def mncut_discretize(Vk, *, k, seed, cfg):\n",
        "    hp = cfg[\"MNCUT\"]\n",
        "    Vk = np.asarray(Vk, dtype=np.float64)\n",
        "    n, kk = Vk.shape\n",
        "    if kk != int(k):\n",
        "        raise ValueError(\"MNCut discretize expects Vk shape (n, k) where k == n_clusters\")\n",
        "\n",
        "    Vn = Vk / (np.linalg.norm(Vk, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    rng = np.random.RandomState(int(seed))\n",
        "    A = rng.normal(size=(int(k), int(k)))\n",
        "    Q, _ = np.linalg.qr(A)\n",
        "    R = Q\n",
        "\n",
        "    labels_prev = None\n",
        "    for _it in range(int(hp[\"MAX_ITER\"])):\n",
        "        Y = Vn @ R\n",
        "        labels = np.argmax(Y, axis=1).astype(np.int32)\n",
        "\n",
        "        counts = np.bincount(labels, minlength=int(k))\n",
        "        empties = np.where(counts == 0)[0]\n",
        "        if empties.size:\n",
        "            part = np.partition(Y, -2, axis=1)\n",
        "            margin = part[:, -1] - part[:, -2]\n",
        "            order = np.argsort(margin)\n",
        "            used = set()\n",
        "            ptr = 0\n",
        "            for ek in empties.tolist():\n",
        "                while ptr < n and order[ptr] in used:\n",
        "                    ptr += 1\n",
        "                ridx = int(order[ptr]) if ptr < n else int(rng.randint(0, n))\n",
        "                used.add(ridx)\n",
        "                labels[ridx] = int(ek)\n",
        "\n",
        "        if labels_prev is not None:\n",
        "            change = float(np.mean(labels != labels_prev))\n",
        "            if change <= float(hp[\"TOL\"]):\n",
        "                break\n",
        "        labels_prev = labels.copy()\n",
        "\n",
        "        E = np.zeros((n, int(k)), dtype=np.float64)\n",
        "        E[np.arange(n), labels] = 1.0\n",
        "\n",
        "        M = E.T @ Vn\n",
        "        U, _, Vt = np.linalg.svd(M, full_matrices=False)\n",
        "        R = Vt.T @ U.T\n",
        "\n",
        "    return labels.astype(np.int32)\n",
        "\n",
        "def _export_clusters_one_csv(\n",
        "    out_dir: Path,\n",
        "    *,\n",
        "    verbs: list[str],\n",
        "    V: int,\n",
        "    active_ids: np.ndarray,\n",
        "    seeds: list[int],\n",
        "    labels_npz_fn,\n",
        "    filename: str = \"clusters.csv\",\n",
        "):\n",
        "    out_dir = _ensure_dir(out_dir)\n",
        "    clusters_csv = out_dir / filename\n",
        "\n",
        "    df = pd.DataFrame({\"verb\": verbs})\n",
        "\n",
        "    for i, seed in enumerate(seeds):\n",
        "        p = labels_npz_fn(int(seed))\n",
        "        if not p.exists():\n",
        "            raise RuntimeError(f\"Missing labels checkpoint for seed={seed}: {p}\")\n",
        "\n",
        "        z = np.load(p, allow_pickle=False)\n",
        "        labels_active = z[\"labels\"].astype(np.int32)\n",
        "\n",
        "        full_labels = np.full(V, -1, dtype=np.int32)\n",
        "        full_labels[active_ids] = labels_active\n",
        "\n",
        "        df[f\"cluster_{i}\"] = full_labels\n",
        "\n",
        "    _atomic_write_df_csv(df, clusters_csv)\n",
        "\n",
        "def _assemble_matrix(feature_names: list[str], mats: dict[str, sp.csr_matrix]):\n",
        "    blocks = [mats[f] for f in feature_names]\n",
        "    return blocks[0] if len(blocks) == 1 else sp.hstack(blocks, format=\"csr\")\n",
        "\n",
        "def run_one_subset_resumable(\n",
        "    *,\n",
        "    out_dir: Path,\n",
        "    verbs: list[str],\n",
        "    V: int,\n",
        "    X: sp.csr_matrix,\n",
        "    feature_names: list[str],\n",
        "    cfg: dict,\n",
        "    knn: int,\n",
        "    n_clusters: int,\n",
        "    emb_dims: int,\n",
        "    feats_tag_override: str | None = None,\n",
        "    ui: ProgressUI | None = None\n",
        ") -> dict:\n",
        "    out_dir = _ensure_dir(out_dir)\n",
        "\n",
        "    feats_tag = feats_tag_override or _feature_tag(feature_names)\n",
        "    feats_tag = _safe_name(feats_tag)\n",
        "\n",
        "    run_id = _safe_name(f\"{cfg['IO']['RUN_STAMP']}_mncut_knn{knn}_k{n_clusters}_emb{emb_dims}_{feats_tag}\")\n",
        "\n",
        "    fp = _fingerprint(cfg, features=feature_names, knn=knn, k=n_clusters, emb_dims=emb_dims)\n",
        "\n",
        "    embed_npz = out_dir / \"_cache\" / f\"{run_id}__EMBED.npz\"\n",
        "    wdiag_npz = out_dir / \"_cache\" / f\"{run_id}__Wdiag0.npz\"\n",
        "    state_json = out_dir / \"_cache\" / f\"{run_id}__STATE.json\"\n",
        "    seed_metrics_csv = out_dir / \"_cache\" / f\"{run_id}__seed_metrics.csv\"\n",
        "    done_json = out_dir / \"_cache\" / f\"{run_id}__DONE.json\"\n",
        "\n",
        "    n_seeds = int(cfg[\"SEEDS\"][\"N_SEED_RUNS\"])\n",
        "    seeds = _make_seeds(int(cfg[\"SEEDS\"][\"BASE_SEED\"]), n_seeds)\n",
        "\n",
        "    labels_dir = out_dir / \"_cache\" / \"_labels\" / run_id\n",
        "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def labels_npz(seed: int) -> Path:\n",
        "        return labels_dir / f\"labels_seed{seed}.npz\"\n",
        "\n",
        "    active_ids_fast = np.where(X.getnnz(axis=1) > 0)[0].astype(np.int32)\n",
        "\n",
        "    if done_json.exists():\n",
        "        saved = _read_json(done_json) or {}\n",
        "        if saved.get(\"fingerprint\") == fp:\n",
        "            clusters_csv = out_dir / \"clusters.csv\"\n",
        "            if not clusters_csv.exists():\n",
        "                missing = [s for s in seeds if not labels_npz(int(s)).exists()]\n",
        "                if missing:\n",
        "                    raise RuntimeError(f\"DONE.json exists but missing label checkpoints for seeds: {missing}\")\n",
        "                _export_clusters_one_csv(\n",
        "                    out_dir,\n",
        "                    verbs=verbs, V=V, active_ids=active_ids_fast,\n",
        "                    seeds=[int(s) for s in seeds],\n",
        "                    labels_npz_fn=labels_npz,\n",
        "                    filename=\"clusters.csv\",\n",
        "                )\n",
        "\n",
        "            df_seed = _safe_read_csv(seed_metrics_csv)\n",
        "            if not df_seed.empty:\n",
        "                return _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg)\n",
        "\n",
        "    st = _read_json(state_json)\n",
        "    if st and st.get(\"fingerprint\") and st.get(\"fingerprint\") != fp:\n",
        "        raise RuntimeError(\n",
        "            \"Found existing checkpoints for this run_id, but config changed.\\n\"\n",
        "            \"Use a new OUT_DIR or change RUN_STAMP (or delete _cache for this run_id).\"\n",
        "        )\n",
        "\n",
        "    if ui:\n",
        "        ui.start_run(2 + n_seeds, f\"Run: {run_id}\")\n",
        "\n",
        "    cached = _load_embed_cache(embed_npz, wdiag_npz, fp)\n",
        "    if cached is not None:\n",
        "        active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used = cached\n",
        "        if ui: ui.step_run(1, \"Loaded cached embedding.\")\n",
        "    else:\n",
        "        active_ids, Z, knn_idx, knn_sim, W_diag0, emb_dim_used = build_graph_and_embedding(\n",
        "            X, cfg=cfg, knn=knn, emb_dims=emb_dims, ui=ui\n",
        "        )\n",
        "        _save_embed_cache(embed_npz, wdiag_npz, fp=fp,\n",
        "                          active_ids=active_ids, Z=Z,\n",
        "                          knn_idx=knn_idx, knn_sim=knn_sim,\n",
        "                          emb_dim_used=emb_dim_used, W_diag0=W_diag0)\n",
        "        if ui: ui.step_run(1, \"Saved embedding cache.\")\n",
        "\n",
        "    n = Z.shape[0]\n",
        "    k_used = int(min(int(n_clusters), n - 1))\n",
        "    if k_used < 2:\n",
        "        raise RuntimeError(\"k_used<2\")\n",
        "    if k_used > Z.shape[1]:\n",
        "        raise RuntimeError(f\"MNCut needs emb_dims >= k_used (got emb={Z.shape[1]}, k_used={k_used})\")\n",
        "\n",
        "    _atomic_write_json(state_json, {\n",
        "        \"run_id\": run_id,\n",
        "        \"fingerprint\": fp,\n",
        "        \"features\": list(feature_names),\n",
        "        \"knn\": int(knn),\n",
        "        \"k\": int(n_clusters),\n",
        "        \"k_used\": int(k_used),\n",
        "        \"emb_dims\": int(emb_dims),\n",
        "        \"emb_dim_used\": int(emb_dim_used),\n",
        "        \"n_seeds\": int(n_seeds),\n",
        "        \"seeds\": list(seeds),\n",
        "        \"updated\": _now_str(),\n",
        "    })\n",
        "\n",
        "    df_seed = _safe_read_csv(seed_metrics_csv)\n",
        "    done_seeds = set(df_seed[\"seed\"].astype(int).tolist()) if (\"seed\" in df_seed.columns and not df_seed.empty) else set()\n",
        "\n",
        "    labels_list = []\n",
        "    for s in seeds:\n",
        "        p = labels_npz(int(s))\n",
        "        if p.exists():\n",
        "            try:\n",
        "                z = np.load(p, allow_pickle=False)\n",
        "                labels_list.append(z[\"labels\"].astype(np.int32))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    done_seeds = {int(s) for s in done_seeds if labels_npz(int(s)).exists()}\n",
        "    remaining = [s for s in seeds if int(s) not in done_seeds]\n",
        "\n",
        "    if ui: ui.step_run(0, f\"Seeds remaining: {len(remaining)}/{len(seeds)}\")\n",
        "\n",
        "    Zk = normalize(Z[:, :k_used], axis=1)  # compute once\n",
        "\n",
        "    for si, seed in enumerate(remaining, start=1):\n",
        "        if ui: ui.step_run(0, f\"MNCut seed {si}/{len(remaining)}...\")\n",
        "\n",
        "        labels = mncut_discretize(Zk, k=k_used, seed=int(seed), cfg=cfg).astype(np.int32)\n",
        "\n",
        "        _atomic_save_npz_array(labels_npz(int(seed)), labels=labels)\n",
        "\n",
        "        purity = _neighbor_purity_from_knn(knn_idx, knn_sim, labels)\n",
        "        Q = _modularity_Q(W_diag0, labels)\n",
        "        ncut = _multiway_ncut(W_diag0, labels)\n",
        "        row = {\n",
        "            \"run_id\": run_id,\n",
        "            \"seed\": int(seed),\n",
        "            \"active_verbs\": int(n),\n",
        "            \"k_used\": int(k_used),\n",
        "            \"emb_dim_used\": int(emb_dim_used),\n",
        "            \"neighbor_purity\": float(purity),\n",
        "            \"modularity_Q\": float(Q),\n",
        "            \"ncut\": float(ncut),\n",
        "            \"score_purity_plus_Q\": float(purity + Q),\n",
        "        }\n",
        "        df_seed = pd.concat([df_seed, pd.DataFrame([row])], ignore_index=True)\n",
        "        df_seed = df_seed.sort_values([\"seed\"]).reset_index(drop=True)\n",
        "        _atomic_write_df_csv(df_seed, seed_metrics_csv)\n",
        "\n",
        "        labels_list.append(labels.copy())\n",
        "        if ui: ui.step_run(1)\n",
        "\n",
        "    nmi_mean, nmi_std = _nmi_stability(labels_list, cfg)\n",
        "\n",
        "    _atomic_write_json(done_json, {\"done\": True, \"when\": _now_str(), \"fingerprint\": fp})\n",
        "\n",
        "    _export_clusters_one_csv(\n",
        "        out_dir,\n",
        "        verbs=verbs,\n",
        "        V=V,\n",
        "        active_ids=active_ids,\n",
        "        seeds=[int(s) for s in seeds],\n",
        "        labels_npz_fn=labels_npz,\n",
        "        filename=\"clusters.csv\",\n",
        "    )\n",
        "\n",
        "    return _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg, nmi_mean, nmi_std)\n",
        "\n",
        "def _neighbor_purity_from_knn(knn_idx, knn_sim, labels):\n",
        "    n = labels.shape[0]\n",
        "    pur = np.zeros(n, dtype=np.float64)\n",
        "    for i in range(n):\n",
        "        inds = knn_idx[i]\n",
        "        sims = knn_sim[i]\n",
        "        m = inds != i\n",
        "        inds = inds[m]; sims = sims[m]\n",
        "        if inds.size == 0:\n",
        "            pur[i] = np.nan\n",
        "            continue\n",
        "        sims = np.maximum(sims, 0.0)\n",
        "        tot = float(sims.sum())\n",
        "        if tot <= 0:\n",
        "            pur[i] = np.nan\n",
        "            continue\n",
        "        inside = float(sims[labels[inds] == labels[i]].sum())\n",
        "        pur[i] = inside / tot\n",
        "    return float(np.nanmean(pur))\n",
        "\n",
        "def _modularity_Q(W_diag0, labels):\n",
        "    deg = np.asarray(W_diag0.sum(axis=1)).ravel().astype(np.float64)\n",
        "    m = float(W_diag0.sum() / 2.0)\n",
        "    if m <= 0:\n",
        "        return np.nan\n",
        "    two_m = 2.0 * m\n",
        "    Q = 0.0\n",
        "    for c in np.unique(labels):\n",
        "        mask = (labels == c)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        vol_c = float(deg[mask].sum())\n",
        "        internal_twice = float(W_diag0[mask][:, mask].sum())\n",
        "        Q += (internal_twice / two_m) - (vol_c / two_m) ** 2\n",
        "    return float(Q)\n",
        "\n",
        "def _multiway_ncut(W_diag0, labels):\n",
        "    deg = np.asarray(W_diag0.sum(axis=1)).ravel().astype(np.float64)\n",
        "    ncut = 0.0\n",
        "    for c in np.unique(labels):\n",
        "        mask = (labels == c)\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        vol = float(deg[mask].sum())\n",
        "        if vol <= 0:\n",
        "            continue\n",
        "        internal_twice = float(W_diag0[mask][:, mask].sum())\n",
        "        cut = vol - internal_twice\n",
        "        ncut += cut / vol\n",
        "    return float(ncut)\n",
        "\n",
        "def _nmi_stability(labels_list, cfg):\n",
        "    S = len(labels_list)\n",
        "    if S < 2:\n",
        "        return (np.nan, np.nan)\n",
        "\n",
        "    max_pairs = cfg[\"METRICS\"][\"NMI_MAX_PAIRS\"]\n",
        "    pairs = [(i, j) for i in range(S) for j in range(i+1, S)]\n",
        "\n",
        "    if max_pairs is not None and len(pairs) > int(max_pairs):\n",
        "        rng = np.random.RandomState(int(cfg[\"SEEDS\"][\"BASE_SEED\"]) + 999)\n",
        "        idx = rng.choice(len(pairs), size=int(max_pairs), replace=False)\n",
        "        pairs = [pairs[t] for t in idx.tolist()]\n",
        "\n",
        "    vals = []\n",
        "    for i, j in pairs:\n",
        "        vals.append(normalized_mutual_info_score(labels_list[i], labels_list[j], average_method=\"arithmetic\"))\n",
        "\n",
        "    vals = np.array(vals, dtype=np.float64)\n",
        "    return float(np.mean(vals)), float(np.std(vals))\n",
        "\n",
        "def _summarize_from_seed_df(run_id, feature_names, knn, n_clusters, emb_dims, df_seed, fp, cfg, nmi_mean=None, nmi_std=None):\n",
        "    def _mean(x): return float(np.nanmean(x))\n",
        "    def _std(x): return float(np.nanstd(x))\n",
        "\n",
        "    if nmi_mean is None or nmi_std is None:\n",
        "        nmi_mean = np.nan\n",
        "        nmi_std = np.nan\n",
        "\n",
        "    row = {\n",
        "        \"run_id\": str(run_id),\n",
        "        \"features\": \" | \".join(feature_names),\n",
        "        \"n_features\": int(len(feature_names)),\n",
        "        \"knn\": int(knn),\n",
        "        \"k\": int(n_clusters),\n",
        "        \"emb_dims\": int(emb_dims),\n",
        "        \"active_verbs\": int(df_seed[\"active_verbs\"].iloc[0]) if (not df_seed.empty and \"active_verbs\" in df_seed.columns) else np.nan,\n",
        "        \"n_seeds\": int(len(df_seed)),\n",
        "\n",
        "        \"neighbor_purity_mean\": _mean(df_seed[\"neighbor_purity\"]),\n",
        "        \"neighbor_purity_std\": _std(df_seed[\"neighbor_purity\"]),\n",
        "        \"modularity_Q_mean\": _mean(df_seed[\"modularity_Q\"]),\n",
        "        \"modularity_Q_std\": _std(df_seed[\"modularity_Q\"]),\n",
        "        \"ncut_mean\": _mean(df_seed[\"ncut\"]),\n",
        "        \"ncut_std\": _std(df_seed[\"ncut\"]),\n",
        "        \"score_mean\": _mean(df_seed[\"score_purity_plus_Q\"]),\n",
        "        \"score_std\": _std(df_seed[\"score_purity_plus_Q\"]),\n",
        "\n",
        "        \"nmi_stability_mean\": float(nmi_mean),\n",
        "        \"nmi_stability_std\": float(nmi_std),\n",
        "\n",
        "        \"fingerprint\": str(fp),\n",
        "        \"run_stamp\": str(cfg[\"IO\"][\"RUN_STAMP\"]),\n",
        "        \"updated\": _now_str(),\n",
        "    }\n",
        "    return row\n",
        "\n",
        "def make_config(\n",
        "    *,\n",
        "    base_dir: str,\n",
        "    out_dir: str,\n",
        "    run_stamp: str | None = None,\n",
        "    n_seed_runs: int = 10,\n",
        "    base_seed: int = 0,\n",
        "    emb_random_state: int = 0,\n",
        "    use_col: str = \"total_occurrences\",\n",
        "    min_count: float = 0.0,\n",
        "    chunk: int = 1_000_000,\n",
        "    verbverb_mode: str = \"both\",\n",
        "    weight_transform: str = \"raw\",\n",
        "    graph: dict | None = None,\n",
        "    mncut_hp: dict | None = None,\n",
        "    nmi_max_pairs: int | None = None,\n",
        "    ui: dict | None = None,\n",
        "):\n",
        "    _graph = dict(\n",
        "        KNN_METRIC=\"cosine\",\n",
        "        N_JOBS=-1,\n",
        "        SIM_MIN=0.0,\n",
        "        MAKE_SYMMETRIC=True,\n",
        "        DIAG_EPS=1e-6,\n",
        "        MIN_ACTIVE_VERBS=5,\n",
        "    )\n",
        "    if graph:\n",
        "        _graph.update(graph)\n",
        "\n",
        "    _mncut = dict(MAX_ITER=50, TOL=1e-6)\n",
        "    if mncut_hp:\n",
        "        _mncut.update(mncut_hp)\n",
        "\n",
        "    if ui is None:\n",
        "        ui = {\"ENABLE\": True, \"SHOW_LOG\": False}\n",
        "\n",
        "    cfg = {\n",
        "        \"UI\": ui,\n",
        "        \"IO\": {\n",
        "            \"OUT_DIR\": out_dir,\n",
        "            \"RUN_STAMP\": run_stamp,\n",
        "        },\n",
        "        \"DATA\": {\n",
        "            \"VERB_LIST_CSV\": str(Path(base_dir) / \"Wiktionary_Thai_verb_26122025.csv\"),\n",
        "            \"USE_COL\": use_col,\n",
        "            \"MIN_COUNT\": float(min_count),\n",
        "            \"CHUNK\": int(chunk),\n",
        "            \"VERBVERB_MODE\": verbverb_mode,\n",
        "            \"WEIGHT_TRANSFORM\": weight_transform,\n",
        "        },\n",
        "        \"SEEDS\": {\n",
        "            \"BASE_SEED\": int(base_seed),\n",
        "            \"N_SEED_RUNS\": int(n_seed_runs),\n",
        "            \"EMB_RANDOM_STATE\": int(emb_random_state),\n",
        "        },\n",
        "        \"GRAPH\": _graph,\n",
        "        \"MNCUT\": _mncut,\n",
        "        \"METRICS\": {\"NMI_MAX_PAIRS\": nmi_max_pairs},\n",
        "    }\n",
        "    return cfg\n",
        "\n",
        "def load_selected_feature_mats(*, base_dir: str, vid: dict, V: int, cfg: dict,\n",
        "                               feature_on: dict[str, bool]) -> dict[str, sp.csr_matrix]:\n",
        "    specs = default_feature_specs(base_dir)\n",
        "    chosen = [k for k, v in feature_on.items() if bool(v)]\n",
        "    chosen = [k for k in chosen if k in specs]\n",
        "    if not chosen:\n",
        "        raise RuntimeError(\"No features selected. Set at least one FEATURE_ON[name]=True\")\n",
        "\n",
        "    mats = {}\n",
        "    for f in chosen:\n",
        "        spec = specs[f]\n",
        "        path = Path(spec[\"path\"])\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Missing feature file: {path}\")\n",
        "\n",
        "        kind = spec[\"kind\"]\n",
        "        if kind == \"verbverb\":\n",
        "            A = read_verb_verb_matrix(\n",
        "                path,\n",
        "                vid=vid, V=V,\n",
        "                prefix_col=spec.get(\"prefix_col\", \"prefix\"),\n",
        "                suffix_col=spec.get(\"suffix_col\", \"suffix\"),\n",
        "                use_col=cfg[\"DATA\"][\"USE_COL\"],\n",
        "                min_count=cfg[\"DATA\"][\"MIN_COUNT\"],\n",
        "                chunk=cfg[\"DATA\"][\"CHUNK\"],\n",
        "                weight_transform=cfg[\"DATA\"][\"WEIGHT_TRANSFORM\"],\n",
        "            )\n",
        "            X = apply_verbverb_mode(A, cfg[\"DATA\"][\"VERBVERB_MODE\"])\n",
        "        elif kind == \"verbctx\":\n",
        "            X = read_verb_context_matrix(\n",
        "                path,\n",
        "                vid=vid, V=V,\n",
        "                verb_col=spec[\"verb_col\"],\n",
        "                ctx_col=spec[\"ctx_col\"],\n",
        "                use_col=cfg[\"DATA\"][\"USE_COL\"],\n",
        "                min_count=cfg[\"DATA\"][\"MIN_COUNT\"],\n",
        "                chunk=cfg[\"DATA\"][\"CHUNK\"],\n",
        "                weight_transform=cfg[\"DATA\"][\"WEIGHT_TRANSFORM\"],\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown feature kind: {kind}\")\n",
        "\n",
        "        X = X.tocsr()\n",
        "        X.sum_duplicates()\n",
        "        mats[f] = X\n",
        "\n",
        "    return mats\n",
        "\n",
        "def run_experiment(\n",
        "    *,\n",
        "    base_dir: str,\n",
        "    out_dir: str,\n",
        "    feature_on: dict[str, bool],\n",
        "    knn: int,\n",
        "    n_clusters: int,\n",
        "    emb_dims: int,\n",
        "    cfg: dict\n",
        ") -> pd.DataFrame:\n",
        "    ui_cfg = cfg.get(\"UI\", {})\n",
        "    ui = ProgressUI(enabled=bool(ui_cfg.get(\"ENABLE\", True)), show_log=bool(ui_cfg.get(\"SHOW_LOG\", False)))\n",
        "\n",
        "    out_path = _ensure_dir(Path(out_dir))\n",
        "\n",
        "    stamp_file = out_path / \".RUN_STAMP.txt\"\n",
        "    if cfg[\"IO\"][\"RUN_STAMP\"]:\n",
        "        run_stamp = str(cfg[\"IO\"][\"RUN_STAMP\"])\n",
        "        if not stamp_file.exists():\n",
        "            _atomic_write_text(stamp_file, run_stamp)\n",
        "    else:\n",
        "        if stamp_file.exists():\n",
        "            run_stamp = stamp_file.read_text(encoding=\"utf-8\").strip()\n",
        "        else:\n",
        "            run_stamp = _now_str()\n",
        "            _atomic_write_text(stamp_file, run_stamp)\n",
        "    cfg[\"IO\"][\"RUN_STAMP\"] = run_stamp\n",
        "\n",
        "    ui.set_status(\"Loading verbs...\")\n",
        "    verbs, vid = load_verbs(Path(cfg[\"DATA\"][\"VERB_LIST_CSV\"]))\n",
        "    V = len(verbs)\n",
        "    print(\"Loaded verbs:\", V)\n",
        "\n",
        "    ui.set_status(\"Loading selected feature matrices...\")\n",
        "    mats = load_selected_feature_mats(base_dir=base_dir, vid=vid, V=V, cfg=cfg, feature_on=feature_on)\n",
        "    feature_names = list(mats.keys())\n",
        "    print(\"Selected features:\", feature_names)\n",
        "\n",
        "    ui.set_status(\"Assembling matrix...\")\n",
        "    X = _assemble_matrix(feature_names, mats)\n",
        "\n",
        "    ui.start_overall(1, \"Running MNCut (resumable)...\")\n",
        "\n",
        "    all_features = list(default_feature_specs(base_dir).keys())\n",
        "    feats_tag_override = _feature_tag(feature_names, all_feature_names=all_features)\n",
        "\n",
        "    summary = run_one_subset_resumable(\n",
        "        out_dir=out_path,\n",
        "        verbs=verbs,\n",
        "        V=V,\n",
        "        X=X,\n",
        "        feature_names=feature_names,\n",
        "        cfg=cfg,\n",
        "        knn=int(knn),\n",
        "        n_clusters=int(n_clusters),\n",
        "        emb_dims=int(emb_dims),\n",
        "        feats_tag_override=feats_tag_override,\n",
        "        ui=ui\n",
        "    )\n",
        "\n",
        "    results_csv = out_path / \"master_results.csv\"\n",
        "    df_existing = _safe_read_csv(results_csv)\n",
        "    if not df_existing.empty and \"run_id\" in df_existing.columns:\n",
        "        df_existing = df_existing[df_existing[\"run_id\"].astype(str) != str(summary[\"run_id\"])].copy()\n",
        "        df_out = pd.concat([df_existing, pd.DataFrame([summary])], ignore_index=True)\n",
        "    else:\n",
        "        df_out = pd.DataFrame([summary])\n",
        "\n",
        "    _atomic_write_df_csv(df_out, results_csv)\n",
        "\n",
        "    ui.step_overall(1, \"Finished.\")\n",
        "    ui.mark_done(f\"DONE. Exported: {results_csv} and {out_path / 'clusters.csv'}\")\n",
        "    return df_out\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab_Datasets/VV\"\n",
        "OUT_DIR  = f\"{BASE_DIR}/VV_clusters\" ###\n",
        "\n",
        "KNN = 10\n",
        "N_CLUSTERS = 100\n",
        "EMB_DIMS = 200\n",
        "\n",
        "N_SEED_RUNS = 100\n",
        "BASE_SEED = 0\n",
        "\n",
        "FEATURE_ON = {\n",
        "    \"verb-verb\": True, ###\n",
        "    \"verb-(noun)-verb\": False, ###\n",
        "    \"verb-(verb)-verb\": False, ###\n",
        "    \"verb-(adjective)-verb\": False, ###\n",
        "    \"verb-(adverb)-verb\": False, ###\n",
        "    \"verb-(pronoun)-verb\": False, ###\n",
        "    \"noun-verb\": False, ###\n",
        "    \"adjective-verb\": False, ###\n",
        "    \"adverb-verb\": False, ###\n",
        "    \"pronoun-verb\": False, ###\n",
        "    \"verb-noun\": False, ###\n",
        "    \"verb-adjective\": False, ###\n",
        "    \"verb-adverb\": False, ###\n",
        "    \"verb-pronoun\": False, ###\n",
        "}\n",
        "\n",
        "MNCUT_HP = {\"MAX_ITER\": 50, \"TOL\": 1e-6}\n",
        "UI = {\"ENABLE\": True, \"SHOW_LOG\": False}\n",
        "\n",
        "cfg = make_config(\n",
        "    base_dir=BASE_DIR,\n",
        "    out_dir=OUT_DIR,\n",
        "    run_stamp=None,\n",
        "    n_seed_runs=N_SEED_RUNS,\n",
        "    base_seed=BASE_SEED,\n",
        "    emb_random_state=0,\n",
        "    min_count=0.0,\n",
        "    chunk=1_000_000,\n",
        "    verbverb_mode=\"both\", ###\n",
        "    weight_transform=\"raw\",\n",
        "    mncut_hp=MNCUT_HP,\n",
        "    nmi_max_pairs=None,\n",
        "    ui=UI,\n",
        ")\n",
        "\n",
        "df_results = run_experiment(\n",
        "    base_dir=BASE_DIR,\n",
        "    out_dir=OUT_DIR,\n",
        "    feature_on=FEATURE_ON,\n",
        "    knn=KNN,\n",
        "    n_clusters=N_CLUSTERS,\n",
        "    emb_dims=EMB_DIMS,\n",
        "    cfg=cfg\n",
        ")\n",
        "\n",
        "display(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wisesight sentiment analysis downstream task**"
      ],
      "metadata": {
        "id": "CiEDkMk2adKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas numpy scikit-learn tqdm tensorflow requests\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab_Datasets/VV\"\n",
        "VV_DIR = Path(BASE_DIR)\n",
        "\n",
        "SENT_DIR = VV_DIR / \"sentiment_eval\"\n",
        "SENT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CLUSTER_DIR = VV_DIR / \"VV_clusters\" ###\n",
        "CLUSTERS_CSV = CLUSTER_DIR / \"clusters.csv\"\n",
        "\n",
        "VERB_CSV    = VV_DIR / \"Wiktionary_Thai_verb_26122025.csv\"\n",
        "NOUN_CSV    = VV_DIR / \"Wiktionary_Thai_noun_27122025.csv\"\n",
        "PRONOUN_CSV = VV_DIR / \"Wiktionary_Thai_pronoun_02012026.csv\"\n",
        "ADJ_CSV     = VV_DIR / \"Wiktionary_Thai_adjective_02012026.csv\"\n",
        "ADV_CSV     = VV_DIR / \"Wiktionary_Thai_adverb_02012026.csv\"\n",
        "\n",
        "for p in [VERB_CSV, NOUN_CSV, PRONOUN_CSV, ADJ_CSV, ADV_CSV]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing dictionary file: {p}\")\n",
        "\n",
        "def file_md5_8(p: Path) -> str:\n",
        "    h = hashlib.md5()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:8]\n",
        "\n",
        "EXP_PREFIX = \"VV\" ###\n",
        "\n",
        "RUN_BASELINES = [\"b0\",\"b1\",\"b2\",\"b3\",\"b4\"]\n",
        "\n",
        "RUN_TESTS = [\"t1\", \"t2\", \"t3\", \"t4\"]\n",
        "\n",
        "RUN_BASELINES = [x.strip().lower() for x in RUN_BASELINES]\n",
        "RUN_TESTS     = [x.strip().lower() for x in RUN_TESTS]\n",
        "RUN_BASELINES = [x for x in RUN_BASELINES if x in {\"b0\",\"b1\",\"b2\",\"b3\",\"b4\"}]\n",
        "RUN_TESTS     = [x for x in RUN_TESTS if x in {\"t1\",\"t2\",\"t3\",\"t4\"}]\n",
        "\n",
        "USE_QUESTION_CLASS = True\n",
        "\n",
        "TEST_SIZE = 0.10\n",
        "VAL_SIZE  = 0.10\n",
        "\n",
        "MAX_SAMPLES = None\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 10\n",
        "PATIENCE = 1\n",
        "EMBED_DIM = 256\n",
        "LSTM_UNITS = 256\n",
        "DROPOUT = 0.25\n",
        "\n",
        "K_RANDOM = 100\n",
        "\n",
        "N_SEEDS = 100\n",
        "BASE_SEED_FOR_LIST = 20000\n",
        "\n",
        "BASELINE_PREFIX = \"baseline\"\n",
        "BASELINE_RUNS_CSV   = SENT_DIR / f\"{BASELINE_PREFIX}__runs.csv\"\n",
        "BASELINE_SUMMARY_CSV= SENT_DIR / f\"{BASELINE_PREFIX}__summary.csv\"\n",
        "\n",
        "EXP_RUNS_CSV        = SENT_DIR / f\"{EXP_PREFIX}__runs.csv\"\n",
        "\n",
        "PARSE_CACHE = SENT_DIR / \"_cache_greedy_parse.pkl\"\n",
        "META_CACHE  = SENT_DIR / \"_cache_greedy_parse.meta.json\"\n",
        "\n",
        "WISESIGHT_DIR = SENT_DIR / \"_wisesight_download\"\n",
        "WISESIGHT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "WISESIGHT_URLS = {\n",
        "    \"pos\": \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/pos.txt\",\n",
        "    \"neu\": \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/neu.txt\",\n",
        "    \"neg\": \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/neg.txt\",\n",
        "    \"q\":   \"https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/q.txt\",\n",
        "}\n",
        "\n",
        "def download_file(url: str, path: Path):\n",
        "    import requests\n",
        "    if path.exists() and path.stat().st_size > 0:\n",
        "        return\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    path.write_bytes(r.content)\n",
        "\n",
        "for lab, url in WISESIGHT_URLS.items():\n",
        "    if lab == \"q\" and not USE_QUESTION_CLASS:\n",
        "        continue\n",
        "    download_file(url, WISESIGHT_DIR / f\"{lab}.txt\")\n",
        "\n",
        "def read_lines(p: Path):\n",
        "    out = []\n",
        "    with p.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if s:\n",
        "                out.append(s)\n",
        "    return out\n",
        "\n",
        "labels_to_load = [\"pos\", \"neu\", \"neg\"] + ([\"q\"] if USE_QUESTION_CLASS else [])\n",
        "data = []\n",
        "for lab in labels_to_load:\n",
        "    for s in read_lines(WISESIGHT_DIR / f\"{lab}.txt\"):\n",
        "        data.append((s, lab))\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
        "df = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "if MAX_SAMPLES is not None:\n",
        "    df = df.iloc[:int(MAX_SAMPLES)].copy()\n",
        "\n",
        "label2id = {lab: i for i, lab in enumerate(sorted(df[\"label\"].unique().tolist()))}\n",
        "df[\"y\"] = df[\"label\"].map(label2id).astype(int)\n",
        "\n",
        "print(\"WiseSight size:\", len(df))\n",
        "print(\"Labels:\", label2id)\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "def load_dict_words(csv_path: Path) -> list[str]:\n",
        "    s = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")[0].astype(str)\n",
        "    s = s.str.replace(\"\\ufeff\", \"\", regex=False).str.strip()\n",
        "    s = s[s != \"\"]\n",
        "    return s.tolist()\n",
        "\n",
        "verbs    = load_dict_words(VERB_CSV)\n",
        "nouns    = load_dict_words(NOUN_CSV)\n",
        "pronouns = load_dict_words(PRONOUN_CSV)\n",
        "adjs     = load_dict_words(ADJ_CSV)\n",
        "advs     = load_dict_words(ADV_CSV)\n",
        "\n",
        "verb_set = set(verbs)\n",
        "all_words = set(verbs) | set(nouns) | set(pronouns) | set(adjs) | set(advs)\n",
        "\n",
        "print(\"Dict sizes:\",\n",
        "      \"verbs\", len(verbs),\n",
        "      \"nouns\", len(nouns),\n",
        "      \"pronouns\", len(pronouns),\n",
        "      \"adjs\", len(adjs),\n",
        "      \"advs\", len(advs),\n",
        "      \"ALL\", len(all_words))\n",
        "\n",
        "class TrieNode:\n",
        "    __slots__ = (\"ch\", \"end\")\n",
        "    def __init__(self):\n",
        "        self.ch = {}\n",
        "        self.end = False\n",
        "\n",
        "def build_trie(words):\n",
        "    root = TrieNode()\n",
        "    for w in words:\n",
        "        node = root\n",
        "        for c in w:\n",
        "            node = node.ch.setdefault(c, TrieNode())\n",
        "        node.end = True\n",
        "    return root\n",
        "\n",
        "TRIE = build_trie(all_words)\n",
        "\n",
        "def greedy_tokenize(text: str, trie_root: TrieNode) -> list[str]:\n",
        "    s = re.sub(r\"\\s+\", \"\", str(text))\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    unknown_run = False\n",
        "\n",
        "    while i < len(s):\n",
        "        node = trie_root\n",
        "        j = i\n",
        "        last_end = -1\n",
        "\n",
        "        while j < len(s) and s[j] in node.ch:\n",
        "            node = node.ch[s[j]]\n",
        "            j += 1\n",
        "            if node.end:\n",
        "                last_end = j\n",
        "\n",
        "        if last_end != -1:\n",
        "            if unknown_run:\n",
        "                tokens.append(\"<OTHER>\")\n",
        "                unknown_run = False\n",
        "            tokens.append(s[i:last_end])\n",
        "            i = last_end\n",
        "        else:\n",
        "            unknown_run = True\n",
        "            i += 1\n",
        "\n",
        "    if unknown_run:\n",
        "        tokens.append(\"<OTHER>\")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def make_parse_meta():\n",
        "    return {\n",
        "        \"dataset_size\": int(len(df)),\n",
        "        \"labels\": sorted(df[\"label\"].unique().tolist()),\n",
        "        \"dict_mtime\": {\n",
        "            \"verb\": VERB_CSV.stat().st_mtime,\n",
        "            \"noun\": NOUN_CSV.stat().st_mtime,\n",
        "            \"pronoun\": PRONOUN_CSV.stat().st_mtime,\n",
        "            \"adjective\": ADJ_CSV.stat().st_mtime,\n",
        "            \"adverb\": ADV_CSV.stat().st_mtime,\n",
        "        },\n",
        "        \"dict_sizes\": {\n",
        "            \"verbs\": int(len(verbs)),\n",
        "            \"nouns\": int(len(nouns)),\n",
        "            \"pronouns\": int(len(pronouns)),\n",
        "            \"adjs\": int(len(adjs)),\n",
        "            \"advs\": int(len(advs)),\n",
        "            \"all\": int(len(all_words)),\n",
        "        },\n",
        "        \"tokenizer\": \"greedy_trie_longest_match__collapse_unknown_to_OTHER\",\n",
        "        \"use_question_class\": bool(USE_QUESTION_CLASS),\n",
        "        \"max_samples\": int(MAX_SAMPLES) if MAX_SAMPLES is not None else None,\n",
        "    }\n",
        "\n",
        "def parse_cache_valid() -> bool:\n",
        "    if not (PARSE_CACHE.exists() and META_CACHE.exists()):\n",
        "        return False\n",
        "    try:\n",
        "        old = json.loads(META_CACHE.read_text(encoding=\"utf-8\"))\n",
        "        new = make_parse_meta()\n",
        "        keys = [\"dataset_size\", \"labels\", \"dict_mtime\", \"dict_sizes\", \"tokenizer\", \"use_question_class\", \"max_samples\"]\n",
        "        return all(old.get(k) == new.get(k) for k in keys)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "print(\"Greedy parsing all texts (resumable)...\")\n",
        "if parse_cache_valid():\n",
        "    with PARSE_CACHE.open(\"rb\") as f:\n",
        "        cached = pickle.load(f)\n",
        "    parsed_tokens = cached[\"parsed_tokens\"]\n",
        "    parsed_isverb = cached[\"parsed_isverb\"]\n",
        "    print(\"Loaded parse cache:\", PARSE_CACHE)\n",
        "else:\n",
        "    parsed_tokens = []\n",
        "    parsed_isverb = []\n",
        "    for t in tqdm(df[\"text\"].tolist(), desc=\"Tokenizing\"):\n",
        "        toks = greedy_tokenize(t, TRIE)\n",
        "        parsed_tokens.append(toks)\n",
        "        parsed_isverb.append([tok in verb_set for tok in toks])\n",
        "\n",
        "    with PARSE_CACHE.open(\"wb\") as f:\n",
        "        pickle.dump({\"parsed_tokens\": parsed_tokens, \"parsed_isverb\": parsed_isverb},\n",
        "                    f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    META_CACHE.write_text(json.dumps(make_parse_meta(), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(\"Saved parse cache:\", PARSE_CACHE)\n",
        "\n",
        "def stable_hash(obj: dict) -> str:\n",
        "    blob = json.dumps(obj, sort_keys=True, ensure_ascii=False).encode(\"utf-8\")\n",
        "    return hashlib.md5(blob).hexdigest()[:10]\n",
        "\n",
        "SETUP_TAG = stable_hash({\n",
        "    \"use_question_class\": USE_QUESTION_CLASS,\n",
        "    \"test_size\": TEST_SIZE,\n",
        "    \"val_size\": VAL_SIZE,\n",
        "    \"max_samples\": MAX_SAMPLES,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"patience\": PATIENCE,\n",
        "    \"embed_dim\": EMBED_DIM,\n",
        "    \"lstm_units\": LSTM_UNITS,\n",
        "    \"dropout\": DROPOUT,\n",
        "    \"k_random\": K_RANDOM,\n",
        "    \"n_seeds\": N_SEEDS,\n",
        "    \"labels\": sorted(label2id.keys()),\n",
        "    \"parse_meta\": make_parse_meta(),\n",
        "})\n",
        "print(\"SETUP_TAG:\", SETUP_TAG)\n",
        "\n",
        "RUN_TESTS_SET = set(RUN_TESTS)\n",
        "RUN_BASE_SET  = set(RUN_BASELINES)\n",
        "\n",
        "CLUSTER_TAG = \"NOCLUSTERS\"\n",
        "actual_maps = []\n",
        "\n",
        "if RUN_TESTS_SET:\n",
        "    if not CLUSTERS_CSV.exists():\n",
        "        raise FileNotFoundError(f\"CLUSTERS_CSV not found but tests requested: {CLUSTERS_CSV}\")\n",
        "    CLUSTER_TAG = f\"{CLUSTERS_CSV.stem}_{file_md5_8(CLUSTERS_CSV)}\"\n",
        "\n",
        "    clusters_df = pd.read_csv(CLUSTERS_CSV, encoding=\"utf-8-sig\")\n",
        "    if \"verb\" not in clusters_df.columns:\n",
        "        raise RuntimeError(\"clusters CSV must contain a 'verb' column.\")\n",
        "\n",
        "    cluster_cols = [c for c in clusters_df.columns if c.startswith(\"cluster_\")]\n",
        "    cluster_cols = sorted(cluster_cols, key=lambda x: int(x.split(\"_\")[1]))\n",
        "\n",
        "    if len(cluster_cols) < N_SEEDS:\n",
        "        raise RuntimeError(f\"clusters CSV has {len(cluster_cols)} cluster columns, need at least {N_SEEDS}.\")\n",
        "\n",
        "    cluster_cols = cluster_cols[:N_SEEDS]\n",
        "    for col in cluster_cols:\n",
        "        m = dict(zip(\n",
        "            clusters_df[\"verb\"].astype(str).tolist(),\n",
        "            clusters_df[col].astype(int).tolist()\n",
        "        ))\n",
        "        actual_maps.append(m)\n",
        "\n",
        "    print(\"Loaded cluster columns:\", cluster_cols[:5], \"...\", cluster_cols[-1])\n",
        "\n",
        "print(\"CLUSTER_TAG:\", CLUSTER_TAG)\n",
        "\n",
        "def make_random_cluster_map(seed: int, verb_list: list[str], k: int) -> dict[str, int]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    return {v: int(rng.randint(0, k)) for v in verb_list}\n",
        "\n",
        "random_maps = [make_random_cluster_map(10_000 + i, verbs, K_RANDOM) for i in range(N_SEEDS)]\n",
        "\n",
        "def vclass_token(c: int | None) -> str:\n",
        "    if c is None or int(c) < 0:\n",
        "        return \"<VCLASS_OOV>\"\n",
        "    return f\"<VCLASS_{int(c)}>\"\n",
        "\n",
        "def build_repr(tokens: list[str], isverb: list[bool], vmap: dict[str, int] | None, mode: str) -> list[str]:\n",
        "    if mode == \"baseline0_words\":\n",
        "        return tokens\n",
        "\n",
        "    if vmap is None:\n",
        "        raise ValueError(\"vmap required for this mode\")\n",
        "\n",
        "    if mode == \"other_verb_vclass\":\n",
        "        out = []\n",
        "        for tok, vb in zip(tokens, isverb):\n",
        "            out.append(tok)\n",
        "            if vb:\n",
        "                out.append(vclass_token(vmap.get(tok)))\n",
        "        return out\n",
        "\n",
        "    if mode == \"other_vclass\":\n",
        "        out = []\n",
        "        for tok, vb in zip(tokens, isverb):\n",
        "            if vb:\n",
        "                out.append(vclass_token(vmap.get(tok)))\n",
        "            else:\n",
        "                out.append(tok)\n",
        "        return out\n",
        "\n",
        "    if mode == \"verb_vclass\":\n",
        "        out = []\n",
        "        for tok, vb in zip(tokens, isverb):\n",
        "            if vb:\n",
        "                out.append(tok)\n",
        "                out.append(vclass_token(vmap.get(tok)))\n",
        "        return out if out else [\"<NO_VERB>\"]\n",
        "\n",
        "    if mode == \"vclass_only\":\n",
        "        out = []\n",
        "        for tok, vb in zip(tokens, isverb):\n",
        "            if vb:\n",
        "                out.append(vclass_token(vmap.get(tok)))\n",
        "        return out if out else [\"<NO_VERB>\"]\n",
        "\n",
        "    raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "\n",
        "def build_vocab(token_lists: list[list[str]], min_freq: int = 1) -> dict[str, int]:\n",
        "    from collections import Counter\n",
        "    c = Counter()\n",
        "    for toks in token_lists:\n",
        "        c.update(toks)\n",
        "    vocab = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n",
        "    for tok, cnt in c.items():\n",
        "        if cnt >= min_freq and tok not in vocab:\n",
        "            vocab[tok] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def encode(token_lists: list[list[str]], vocab: dict[str, int], max_len: int):\n",
        "    X = np.full((len(token_lists), max_len), PAD, dtype=np.int32)\n",
        "    for i, toks in enumerate(token_lists):\n",
        "        ids = [vocab.get(t, UNK) for t in toks][:max_len]\n",
        "        X[i, :len(ids)] = ids\n",
        "    return X\n",
        "\n",
        "def make_model(vocab_size: int, n_classes: int, max_len: int,\n",
        "               embed_dim: int, lstm_units: int, dropout: float):\n",
        "    inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    out = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
        "    model = tf.keras.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def run_one_experiment(mode: str, vmap: dict[str, int] | None, seed: int):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.keras.utils.set_random_seed(int(seed))\n",
        "    np.random.seed(int(seed))\n",
        "    random.seed(int(seed))\n",
        "\n",
        "    token_lists = [build_repr(toks, vb, vmap, mode) for toks, vb in zip(parsed_tokens, parsed_isverb)]\n",
        "\n",
        "    y = df[\"y\"].to_numpy()\n",
        "    idx = np.arange(len(y))\n",
        "\n",
        "    idx_train, idx_test = train_test_split(\n",
        "        idx, test_size=TEST_SIZE, random_state=int(seed), stratify=y\n",
        "    )\n",
        "\n",
        "    y_train_full = y[idx_train]\n",
        "    idx_tr, idx_val = train_test_split(\n",
        "        idx_train, test_size=VAL_SIZE, random_state=int(seed), stratify=y_train_full\n",
        "    )\n",
        "\n",
        "    tr_tokens = [token_lists[i] for i in idx_tr]\n",
        "    val_tokens = [token_lists[i] for i in idx_val]\n",
        "    te_tokens = [token_lists[i] for i in idx_test]\n",
        "\n",
        "    vocab = build_vocab(tr_tokens, min_freq=1)\n",
        "\n",
        "    lengths = [len(t) for t in tr_tokens]\n",
        "    max_len = int(np.clip(np.percentile(lengths, 95), 8, 128))\n",
        "\n",
        "    X_tr  = encode(tr_tokens, vocab, max_len)\n",
        "    X_val = encode(val_tokens, vocab, max_len)\n",
        "    X_te  = encode(te_tokens, vocab, max_len)\n",
        "\n",
        "    y_tr  = y[idx_tr]\n",
        "    y_val = y[idx_val]\n",
        "    y_te  = y[idx_test]\n",
        "\n",
        "    model = make_model(\n",
        "        vocab_size=len(vocab),\n",
        "        n_classes=len(label2id),\n",
        "        max_len=max_len,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        lstm_units=LSTM_UNITS,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    cb = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\", patience=PATIENCE, restore_best_weights=True\n",
        "    )]\n",
        "\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=0,\n",
        "        callbacks=cb,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    yhat = np.argmax(model.predict(X_te, batch_size=BATCH_SIZE, verbose=0), axis=1)\n",
        "    acc = float(accuracy_score(y_te, yhat))\n",
        "    f1m = float(f1_score(y_te, yhat, average=\"macro\"))\n",
        "\n",
        "    return {\n",
        "        \"vocab_size\": int(len(vocab)),\n",
        "        \"max_len\": int(max_len),\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_f1\": f1m,\n",
        "    }\n",
        "\n",
        "def read_runs_csv(path: Path) -> pd.DataFrame:\n",
        "    if path.exists():\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
        "        except Exception:\n",
        "            return pd.DataFrame()\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def write_runs_csv(df_runs: pd.DataFrame, path: Path):\n",
        "    tmp = path.with_suffix(path.suffix + f\".tmp{os.getpid()}\")\n",
        "    df_runs.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
        "    os.replace(str(tmp), str(path))\n",
        "\n",
        "def row_key(setup_tag: str, cluster_tag: str, cond: str, seed: int) -> str:\n",
        "    return f\"{setup_tag}||{cluster_tag}||{cond}||{int(seed)}\"\n",
        "\n",
        "TRAIN_SEEDS = [int(BASE_SEED_FOR_LIST + i) for i in range(N_SEEDS)]\n",
        "SEED_SET = set(TRAIN_SEEDS)\n",
        "\n",
        "TEST_TO_BASELINE = {\"t1\":\"b1\", \"t2\":\"b2\", \"t3\":\"b3\", \"t4\":\"b4\"}\n",
        "\n",
        "def require_baselines_complete_for_tests():\n",
        "    if not RUN_TESTS_SET:\n",
        "        return\n",
        "    need = sorted({TEST_TO_BASELINE[t] for t in RUN_TESTS_SET})\n",
        "    base_df = read_runs_csv(BASELINE_RUNS_CSV)\n",
        "    if base_df.empty:\n",
        "        raise RuntimeError(f\"Tests requested {sorted(RUN_TESTS_SET)} but no baselines file found: {BASELINE_RUNS_CSV}\")\n",
        "\n",
        "    missing_report = []\n",
        "    for b in need:\n",
        "        sub = base_df[(base_df.get(\"setup_tag\") == SETUP_TAG) & (base_df.get(\"cond\") == b)]\n",
        "        have = set(sub[\"seed\"].astype(int).tolist()) if not sub.empty and \"seed\" in sub.columns else set()\n",
        "        miss = sorted(SEED_SET - have)\n",
        "        if miss:\n",
        "            missing_report.append((b, len(miss), miss[:10]))\n",
        "\n",
        "    if missing_report:\n",
        "        msg = \"Cannot run tests: required baselines are incomplete for this SETUP_TAG.\\n\"\n",
        "        msg += f\"SETUP_TAG={SETUP_TAG}\\n\"\n",
        "        msg += \"Missing seeds (showing first 10):\\n\"\n",
        "        for b, nmiss, first10 in missing_report:\n",
        "            msg += f\"  {b}: missing {nmiss} seeds, e.g. {first10}\\n\"\n",
        "        msg += \"\\nRun baselines first (at least b1..b4) until complete.\"\n",
        "        raise RuntimeError(msg)\n",
        "\n",
        "require_baselines_complete_for_tests()\n",
        "\n",
        "COND_INFO = {\n",
        "    \"b0\": (\"baseline0_words\", None),\n",
        "    \"b1\": (\"other_verb_vclass\", \"RAND\"),\n",
        "    \"b2\": (\"other_vclass\", \"RAND\"),\n",
        "    \"b3\": (\"verb_vclass\", \"RAND\"),\n",
        "    \"b4\": (\"vclass_only\", \"RAND\"),\n",
        "\n",
        "    \"t1\": (\"other_verb_vclass\", \"REAL\"),\n",
        "    \"t2\": (\"other_vclass\", \"REAL\"),\n",
        "    \"t3\": (\"verb_vclass\", \"REAL\"),\n",
        "    \"t4\": (\"vclass_only\", \"REAL\"),\n",
        "}\n",
        "\n",
        "def build_plan():\n",
        "    plan = []\n",
        "    for i, seed in enumerate(TRAIN_SEEDS):\n",
        "        for cond in RUN_BASELINES:\n",
        "            mode, kind = COND_INFO[cond]\n",
        "            if cond == \"b0\":\n",
        "                vmap = None\n",
        "                ctag = \"BASELINE\"\n",
        "            else:\n",
        "                vmap = random_maps[i]\n",
        "                ctag = \"BASELINE\"\n",
        "            plan.append((\"BASELINE\", cond, mode, vmap, int(seed), str(ctag)))\n",
        "\n",
        "        if RUN_TESTS_SET:\n",
        "            for cond in RUN_TESTS:\n",
        "                mode, kind = COND_INFO[cond]\n",
        "                vmap = actual_maps[i]  # paired by i\n",
        "                ctag = CLUSTER_TAG\n",
        "                plan.append((EXP_PREFIX, cond, mode, vmap, int(seed), str(ctag)))\n",
        "    return plan\n",
        "\n",
        "plan = build_plan()\n",
        "print(\"Planned runs:\", len(plan))\n",
        "\n",
        "base_df = read_runs_csv(BASELINE_RUNS_CSV)\n",
        "exp_df  = read_runs_csv(EXP_RUNS_CSV)\n",
        "\n",
        "done_base = set()\n",
        "done_exp  = set()\n",
        "\n",
        "for d, keyset in [(base_df, done_base), (exp_df, done_exp)]:\n",
        "    if not d.empty and {\"setup_tag\",\"cluster_tag\",\"cond\",\"seed\"}.issubset(d.columns):\n",
        "        for st, ct, c, s in zip(d[\"setup_tag\"], d[\"cluster_tag\"], d[\"cond\"], d[\"seed\"]):\n",
        "            keyset.add(row_key(str(st), str(ct), str(c), int(s)))\n",
        "\n",
        "for exp_prefix, cond, mode, vmap, seed, ctag in tqdm(plan, desc=\"Training runs (resumable)\"):\n",
        "    key = row_key(SETUP_TAG, ctag, cond, seed)\n",
        "\n",
        "    if exp_prefix == \"BASELINE\":\n",
        "        if key in done_base:\n",
        "            continue\n",
        "    else:\n",
        "        if key in done_exp:\n",
        "            continue\n",
        "\n",
        "    out = run_one_experiment(mode=mode, vmap=vmap, seed=int(seed))\n",
        "\n",
        "    row = {\n",
        "        \"exp_prefix\": exp_prefix,\n",
        "        \"setup_tag\": SETUP_TAG,\n",
        "        \"cluster_tag\": ctag,\n",
        "        \"cond\": cond,\n",
        "        \"mode\": mode,\n",
        "        \"seed\": int(seed),\n",
        "        \"accuracy\": float(out[\"accuracy\"]),\n",
        "        \"macro_f1\": float(out[\"macro_f1\"]),\n",
        "        \"vocab_size\": int(out[\"vocab_size\"]),\n",
        "        \"max_len\": int(out[\"max_len\"]),\n",
        "        \"k_random\": int(K_RANDOM),\n",
        "    }\n",
        "\n",
        "    if exp_prefix == \"BASELINE\":\n",
        "        base_df = pd.concat([base_df, pd.DataFrame([row])], ignore_index=True)\n",
        "        write_runs_csv(base_df, BASELINE_RUNS_CSV)\n",
        "        done_base.add(key)\n",
        "    else:\n",
        "        exp_df = pd.concat([exp_df, pd.DataFrame([row])], ignore_index=True)\n",
        "        write_runs_csv(exp_df, EXP_RUNS_CSV)\n",
        "        done_exp.add(key)\n",
        "\n",
        "    print(f\"{exp_prefix:10s} {cond:2s} seed={seed}  acc={row['accuracy']:.4f}  f1={row['macro_f1']:.4f}  max_len={row['max_len']}  vocab={row['vocab_size']}\")\n",
        "\n",
        "print(\"Saved baseline runs to:\", BASELINE_RUNS_CSV)\n",
        "print(\"Saved exp runs to     :\", EXP_RUNS_CSV)\n",
        "\n",
        "def summarize_metrics(dfsub: pd.DataFrame):\n",
        "    if dfsub.empty:\n",
        "        return dict(n=0, acc_mean=np.nan, acc_std=np.nan, f1_mean=np.nan, f1_std=np.nan)\n",
        "    return dict(\n",
        "        n=int(len(dfsub)),\n",
        "        acc_mean=float(dfsub[\"accuracy\"].mean()),\n",
        "        acc_std=float(dfsub[\"accuracy\"].std(ddof=1)) if len(dfsub) > 1 else 0.0,\n",
        "        f1_mean=float(dfsub[\"macro_f1\"].mean()),\n",
        "        f1_std=float(dfsub[\"macro_f1\"].std(ddof=1)) if len(dfsub) > 1 else 0.0,\n",
        "    )\n",
        "\n",
        "def paired_delta_f1(b_df: pd.DataFrame, t_df: pd.DataFrame):\n",
        "    if b_df.empty or t_df.empty:\n",
        "        return dict(n_paired=0, delta_f1_mean=np.nan, delta_f1_std=np.nan, delta_f1_ci95_lo=np.nan, delta_f1_ci95_hi=np.nan)\n",
        "\n",
        "    b = b_df[[\"seed\",\"macro_f1\"]].rename(columns={\"macro_f1\":\"f1_b\"}).copy()\n",
        "    t = t_df[[\"seed\",\"macro_f1\"]].rename(columns={\"macro_f1\":\"f1_t\"}).copy()\n",
        "\n",
        "    paired = b.merge(t, on=\"seed\", how=\"inner\")\n",
        "    if paired.empty:\n",
        "        return dict(n_paired=0, delta_f1_mean=np.nan, delta_f1_std=np.nan, delta_f1_ci95_lo=np.nan, delta_f1_ci95_hi=np.nan)\n",
        "\n",
        "    d = (paired[\"f1_t\"] - paired[\"f1_b\"]).to_numpy(dtype=float)\n",
        "    n = int(d.shape[0])\n",
        "    mean = float(np.mean(d))\n",
        "    std  = float(np.std(d, ddof=1)) if n > 1 else 0.0\n",
        "    se   = std / np.sqrt(n) if n > 0 else np.nan\n",
        "    ci_lo = mean - 1.96 * se if n > 1 else mean\n",
        "    ci_hi = mean + 1.96 * se if n > 1 else mean\n",
        "    return dict(n_paired=n, delta_f1_mean=mean, delta_f1_std=std, delta_f1_ci95_lo=float(ci_lo), delta_f1_ci95_hi=float(ci_hi))\n",
        "\n",
        "base_cur = base_df[base_df.get(\"setup_tag\") == SETUP_TAG].copy()\n",
        "exp_cur  = exp_df[(exp_df.get(\"setup_tag\") == SETUP_TAG) & (exp_df.get(\"cluster_tag\") == CLUSTER_TAG)].copy()\n",
        "\n",
        "base_rows = []\n",
        "for cond in [\"b0\",\"b1\",\"b2\",\"b3\",\"b4\"]:\n",
        "    sub = base_cur[base_cur.get(\"cond\") == cond].copy()\n",
        "    s = summarize_metrics(sub)\n",
        "    base_rows.append({\"cond\": cond, **s})\n",
        "baseline_summary = pd.DataFrame(base_rows)\n",
        "write_runs_csv(baseline_summary, BASELINE_SUMMARY_CSV)\n",
        "\n",
        "print(\"\\n=== BASELINE SUMMARY (current SETUP_TAG) ===\")\n",
        "print(baseline_summary.to_string(index=False))\n",
        "print(\"Saved:\", BASELINE_SUMMARY_CSV)\n",
        "\n",
        "summary_rows = []\n",
        "pair_order = [(\"t1\",\"b1\"), (\"t2\",\"b2\"), (\"t3\",\"b3\"), (\"t4\",\"b4\")]\n",
        "\n",
        "for tcond, bcond in pair_order:\n",
        "    b_sub = base_cur[base_cur.get(\"cond\") == bcond].copy()\n",
        "    t_sub = exp_cur[exp_cur.get(\"cond\") == tcond].copy()\n",
        "\n",
        "    sb = summarize_metrics(b_sub)\n",
        "    st = summarize_metrics(t_sub)\n",
        "    dd = paired_delta_f1(b_sub, t_sub)\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"pair\": f\"{tcond}-{bcond}\",\n",
        "        \"baseline_cond\": bcond,\n",
        "        \"test_cond\": tcond,\n",
        "        \"baseline_n\": sb[\"n\"],\n",
        "        \"baseline_f1_mean\": sb[\"f1_mean\"],\n",
        "        \"baseline_f1_std\": sb[\"f1_std\"],\n",
        "        \"test_n\": st[\"n\"],\n",
        "        \"test_f1_mean\": st[\"f1_mean\"],\n",
        "        \"test_f1_std\": st[\"f1_std\"],\n",
        "        \"n_paired\": dd[\"n_paired\"],\n",
        "        \"delta_f1_mean\": dd[\"delta_f1_mean\"],\n",
        "        \"delta_f1_std\": dd[\"delta_f1_std\"],\n",
        "        \"delta_f1_ci95_lo\": dd[\"delta_f1_ci95_lo\"],\n",
        "        \"delta_f1_ci95_hi\": dd[\"delta_f1_ci95_hi\"],\n",
        "        \"setup_tag\": SETUP_TAG,\n",
        "        \"cluster_tag\": CLUSTER_TAG,\n",
        "        \"exp_prefix\": EXP_PREFIX,\n",
        "    })\n",
        "\n",
        "exp_summary = pd.DataFrame(summary_rows)\n",
        "\n",
        "EXP_SUMMARY_CSV = SENT_DIR / f\"{EXP_PREFIX}__summary__{CLUSTER_TAG}__{SETUP_TAG}.csv\"\n",
        "write_runs_csv(exp_summary, EXP_SUMMARY_CSV)\n",
        "\n",
        "print(\"\\n=== EXP SUMMARY + PAIRED ΔF1 (test - baseline, same seed) ===\")\n",
        "print(exp_summary.to_string(index=False))\n",
        "print(\"Saved:\", EXP_SUMMARY_CSV)\n",
        "\n",
        "example = \"วันนี้จะไปเที่ยวน้าาา\"\n",
        "toks = greedy_tokenize(example, TRIE)\n",
        "isv = [t in verb_set for t in toks]\n",
        "\n",
        "print(\"\\nExample text:\", example)\n",
        "print(\"Tokens:\", \" \".join(toks))\n",
        "print(\"b0:\", \" \".join(build_repr(toks, isv, None, \"baseline0_words\")))\n",
        "\n",
        "vmap_rand0 = random_maps[0]\n",
        "print(\"b1:\", \" \".join(build_repr(toks, isv, vmap_rand0, \"other_verb_vclass\")))\n",
        "print(\"b2:\", \" \".join(build_repr(toks, isv, vmap_rand0, \"other_vclass\")))\n",
        "print(\"b3:\", \" \".join(build_repr(toks, isv, vmap_rand0, \"verb_vclass\")))\n",
        "print(\"b4:\", \" \".join(build_repr(toks, isv, vmap_rand0, \"vclass_only\")))\n",
        "\n",
        "if RUN_TESTS_SET and actual_maps:\n",
        "    vmap_real0 = actual_maps[0]\n",
        "    print(\"t1:\", \" \".join(build_repr(toks, isv, vmap_real0, \"other_verb_vclass\")))\n",
        "    print(\"t2:\", \" \".join(build_repr(toks, isv, vmap_real0, \"other_vclass\")))\n",
        "    print(\"t3:\", \" \".join(build_repr(toks, isv, vmap_real0, \"verb_vclass\")))\n",
        "    print(\"t4:\", \" \".join(build_repr(toks, isv, vmap_real0, \"vclass_only\")))"
      ],
      "metadata": {
        "id": "SHUWcyvVahA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Centroid finder**"
      ],
      "metadata": {
        "id": "Yxru7jKIbds7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas numpy scipy scikit-learn tqdm pynndescent\n",
        "\n",
        "import os, json, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.manifold import SpectralEmbedding\n",
        "from pynndescent import NNDescent\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/Colab_Datasets/VV\")\n",
        "\n",
        "REL_CLUSTER_PATH = Path(\"../VV/clusters/VV_clusters.csv\") ###\n",
        "CLUSTERS_CSV = (BASE_DIR / REL_CLUSTER_PATH).resolve()\n",
        "\n",
        "OUT_DIR = (BASE_DIR / \"centroid_outputs_VV\").resolve() ###\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED_COL = \"cluster_0\"\n",
        "\n",
        "TOP_N = 10\n",
        "\n",
        "KNN = 10\n",
        "EMB_DIMS = 200\n",
        "K_EXPECTED = 100\n",
        "\n",
        "USE_COL = \"total_occurrences\"\n",
        "MIN_COUNT = 0.0\n",
        "CHUNK = 1_000_000\n",
        "VERBVERB_MODE = \"both\" ###\n",
        "WEIGHT_TRANSFORM = \"raw\"\n",
        "\n",
        "FEATURE_ON = {\n",
        "    \"verb-verb\": True, ###\n",
        "    \"verb-(noun)-verb\": False, ###\n",
        "    \"verb-(verb)-verb\": False, ###\n",
        "    \"verb-(adjective)-verb\": False, ###\n",
        "    \"verb-(adverb)-verb\": False, ###\n",
        "    \"verb-(pronoun)-verb\": False, ###\n",
        "    \"noun-verb\": False, ###\n",
        "    \"adjective-verb\": False, ###\n",
        "    \"adverb-verb\": False, ###\n",
        "    \"pronoun-verb\": False, ###\n",
        "    \"verb-noun\": False, ###\n",
        "    \"verb-adjective\": False, ###\n",
        "    \"verb-adverb\": False, ###\n",
        "    \"verb-pronoun\": False, ###\n",
        "}\n",
        "\n",
        "if not CLUSTERS_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Cluster CSV not found: {CLUSTERS_CSV}\")\n",
        "\n",
        "VERB_LIST_CSV = BASE_DIR / \"Wiktionary_Thai_verb_26122025.csv\"\n",
        "if not VERB_LIST_CSV.exists():\n",
        "    raise FileNotFoundError(f\"Verb list CSV not found: {VERB_LIST_CSV}\")\n",
        "\n",
        "print(\"Using clusters file:\", CLUSTERS_CSV)\n",
        "print(\"Saving outputs to:\", OUT_DIR)\n",
        "\n",
        "embedding_cache_npz = OUT_DIR / \"embedding_cache.npz\"\n",
        "embedding_cache_meta = OUT_DIR / \"embedding_cache.meta.json\"\n",
        "centroids_npz = OUT_DIR / f\"centroids__{SEED_COL}.npz\"\n",
        "centroids_summary_csv = OUT_DIR / f\"centroids_summary__{SEED_COL}.csv\"\n",
        "\n",
        "def default_feature_specs(base_dir: Path):\n",
        "    return {\n",
        "        \"verb-verb\":             {\"path\": base_dir / \"Thaisum_verb-verb_pairs.csv\",             \"kind\": \"verbverb\"},\n",
        "        \"verb-(noun)-verb\":      {\"path\": base_dir / \"Thaisum_verb-(noun)-verb_pairs.csv\",      \"kind\": \"verbverb\"},\n",
        "        \"verb-(verb)-verb\":      {\"path\": base_dir / \"Thaisum_verb-(verb)-verb_pairs.csv\",      \"kind\": \"verbverb\"},\n",
        "        \"verb-(adjective)-verb\": {\"path\": base_dir / \"Thaisum_verb-(adjective)-verb_pairs.csv\", \"kind\": \"verbverb\"},\n",
        "        \"verb-(adverb)-verb\":    {\"path\": base_dir / \"Thaisum_verb-(adverb)-verb_pairs.csv\",    \"kind\": \"verbverb\"},\n",
        "        \"verb-(pronoun)-verb\":   {\"path\": base_dir / \"Thaisum_verb-(pronoun)-verb_pairs.csv\",   \"kind\": \"verbverb\"},\n",
        "\n",
        "        \"noun-verb\":             {\"path\": base_dir / \"Thaisum_noun-verb_pairs.csv\",             \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"noun\"},\n",
        "        \"adjective-verb\":        {\"path\": base_dir / \"Thaisum_adjective-verb_pairs.csv\",        \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adjective\"},\n",
        "        \"adverb-verb\":           {\"path\": base_dir / \"Thaisum_adverb-verb_pairs.csv\",           \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adverb\"},\n",
        "        \"pronoun-verb\":          {\"path\": base_dir / \"Thaisum_pronoun-verb_pairs.csv\",          \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"pronoun\"},\n",
        "\n",
        "        \"verb-noun\":             {\"path\": base_dir / \"Thaisum_verb-noun_pairs.csv\",             \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"noun\"},\n",
        "        \"verb-adjective\":        {\"path\": base_dir / \"Thaisum_verb-adjective_pairs.csv\",        \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adjective\"},\n",
        "        \"verb-adverb\":           {\"path\": base_dir / \"Thaisum_verb-adverb_pairs.csv\",           \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"adverb\"},\n",
        "        \"verb-pronoun\":          {\"path\": base_dir / \"Thaisum_verb-pronoun_pairs.csv\",          \"kind\": \"verbctx\", \"verb_col\": \"verb\", \"ctx_col\": \"pronoun\"},\n",
        "    }\n",
        "\n",
        "def load_verbs(verb_list_csv: Path):\n",
        "    s = pd.read_csv(verb_list_csv, header=None, encoding=\"utf-8-sig\")[0].astype(str)\n",
        "    s = s.str.replace(\"\\ufeff\", \"\", regex=False).str.strip()\n",
        "    verbs = s[s != \"\"].tolist()\n",
        "    vid = {v: i for i, v in enumerate(verbs)}\n",
        "    return verbs, vid\n",
        "\n",
        "verbs, vid = load_verbs(VERB_LIST_CSV)\n",
        "V = len(verbs)\n",
        "print(\"Loaded verbs:\", V)\n",
        "\n",
        "def read_verb_verb_matrix(path: Path, *, vid: dict, V: int,\n",
        "                          prefix_col: str, suffix_col: str,\n",
        "                          use_col: str, min_count: float, chunk: int,\n",
        "                          weight_transform: str):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    rows_parts, cols_parts, data_parts = [], [], []\n",
        "    usecols = [prefix_col, suffix_col, use_col]\n",
        "\n",
        "    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding=\"utf-8-sig\"),\n",
        "                   desc=f\"Reading {path.name}\"):\n",
        "        df = df.dropna()\n",
        "        if df.empty:\n",
        "            continue\n",
        "        df = df[df[use_col] >= min_count]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        r = df[prefix_col].astype(str).map(vid)\n",
        "        c = df[suffix_col].astype(str).map(vid)\n",
        "        m = r.notna() & c.notna()\n",
        "        if not m.any():\n",
        "            continue\n",
        "\n",
        "        r = r[m].astype(np.int32).to_numpy()\n",
        "        c = c[m].astype(np.int32).to_numpy()\n",
        "        w = df.loc[m, use_col].to_numpy(np.float32)\n",
        "\n",
        "        if weight_transform == \"raw\":\n",
        "            pass\n",
        "        elif weight_transform == \"log1p\":\n",
        "            w = np.log1p(w).astype(np.float32)\n",
        "        elif weight_transform == \"sqrt\":\n",
        "            w = np.sqrt(w).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown weight_transform: {weight_transform}\")\n",
        "\n",
        "        rows_parts.append(r); cols_parts.append(c); data_parts.append(w)\n",
        "\n",
        "    if not rows_parts:\n",
        "        raise RuntimeError(f\"No usable edges in {path.name}.\")\n",
        "\n",
        "    rows = np.concatenate(rows_parts)\n",
        "    cols = np.concatenate(cols_parts)\n",
        "    data = np.concatenate(data_parts)\n",
        "\n",
        "    A = sp.coo_matrix((data, (rows, cols)), shape=(V, V), dtype=np.float32).tocsr()\n",
        "    A.sum_duplicates()\n",
        "    A.setdiag(0)\n",
        "    A.eliminate_zeros()\n",
        "    return A\n",
        "\n",
        "def read_verb_context_matrix(path: Path, *, vid: dict, V: int,\n",
        "                            verb_col: str, ctx_col: str,\n",
        "                            use_col: str, min_count: float, chunk: int,\n",
        "                            weight_transform: str):\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    ctx2id = {}\n",
        "    rows_parts, cols_parts, data_parts = [], [], []\n",
        "    usecols = [verb_col, ctx_col, use_col]\n",
        "\n",
        "    for df in tqdm(pd.read_csv(path, usecols=usecols, chunksize=int(chunk), encoding=\"utf-8-sig\"),\n",
        "                   desc=f\"Reading {path.name}\"):\n",
        "        df = df.dropna()\n",
        "        if df.empty:\n",
        "            continue\n",
        "        df = df[df[use_col] >= min_count]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        v = df[verb_col].astype(str).map(vid)\n",
        "        m = v.notna()\n",
        "        if not m.any():\n",
        "            continue\n",
        "\n",
        "        v = v[m].astype(np.int32).to_numpy()\n",
        "        ctx_words = df.loc[m, ctx_col].astype(str).to_numpy()\n",
        "        w = df.loc[m, use_col].to_numpy(np.float32)\n",
        "\n",
        "        if weight_transform == \"raw\":\n",
        "            pass\n",
        "        elif weight_transform == \"log1p\":\n",
        "            w = np.log1p(w).astype(np.float32)\n",
        "        elif weight_transform == \"sqrt\":\n",
        "            w = np.sqrt(w).astype(np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown weight_transform: {weight_transform}\")\n",
        "\n",
        "        c = np.empty_like(v, dtype=np.int32)\n",
        "        for i, cw in enumerate(ctx_words):\n",
        "            j = ctx2id.get(cw)\n",
        "            if j is None:\n",
        "                j = len(ctx2id)\n",
        "                ctx2id[cw] = j\n",
        "            c[i] = j\n",
        "\n",
        "        rows_parts.append(v); cols_parts.append(c); data_parts.append(w)\n",
        "\n",
        "    if not rows_parts:\n",
        "        raise RuntimeError(f\"No usable edges in {path.name}.\")\n",
        "\n",
        "    rows = np.concatenate(rows_parts)\n",
        "    cols = np.concatenate(cols_parts)\n",
        "    data = np.concatenate(data_parts)\n",
        "\n",
        "    X = sp.coo_matrix((data, (rows, cols)), shape=(V, len(ctx2id)), dtype=np.float32).tocsr()\n",
        "    X.sum_duplicates()\n",
        "    return X\n",
        "\n",
        "def apply_verbverb_mode(A: sp.csr_matrix, mode: str):\n",
        "    if mode == \"out\":\n",
        "        return A\n",
        "    if mode == \"in\":\n",
        "        return A.T.tocsr()\n",
        "    if mode == \"both\":\n",
        "        return sp.hstack([A, A.T], format=\"csr\")\n",
        "    raise ValueError(\"VERBVERB_MODE must be out/in/both\")\n",
        "\n",
        "def assemble_matrix(feature_names: list[str], mats: dict[str, sp.csr_matrix]):\n",
        "    blocks = [mats[f] for f in feature_names]\n",
        "    return blocks[0] if len(blocks) == 1 else sp.hstack(blocks, format=\"csr\")\n",
        "\n",
        "specs = default_feature_specs(BASE_DIR)\n",
        "feature_names = [k for k, v in FEATURE_ON.items() if v]\n",
        "if not feature_names:\n",
        "    raise RuntimeError(\"No features selected in FEATURE_ON\")\n",
        "\n",
        "mats = {}\n",
        "for f in feature_names:\n",
        "    spec = specs[f]\n",
        "    p = Path(spec[\"path\"])\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing feature file: {p}\")\n",
        "\n",
        "    if spec[\"kind\"] == \"verbverb\":\n",
        "        A = read_verb_verb_matrix(\n",
        "            p, vid=vid, V=V,\n",
        "            prefix_col=\"prefix\", suffix_col=\"suffix\",\n",
        "            use_col=USE_COL, min_count=MIN_COUNT, chunk=CHUNK,\n",
        "            weight_transform=WEIGHT_TRANSFORM\n",
        "        )\n",
        "        Xf = apply_verbverb_mode(A, VERBVERB_MODE)\n",
        "    else:\n",
        "        Xf = read_verb_context_matrix(\n",
        "            p, vid=vid, V=V,\n",
        "            verb_col=spec[\"verb_col\"], ctx_col=spec[\"ctx_col\"],\n",
        "            use_col=USE_COL, min_count=MIN_COUNT, chunk=CHUNK,\n",
        "            weight_transform=WEIGHT_TRANSFORM\n",
        "        )\n",
        "\n",
        "    Xf = Xf.tocsr()\n",
        "    Xf.sum_duplicates()\n",
        "    mats[f] = Xf\n",
        "\n",
        "X = assemble_matrix(feature_names, mats).tocsr()\n",
        "X.sum_duplicates()\n",
        "print(\"Assembled X shape:\", X.shape, \"nnz:\", X.nnz)\n",
        "print(\"Features used:\", feature_names)\n",
        "\n",
        "def fingerprint():\n",
        "    obj = {\n",
        "        \"features\": feature_names,\n",
        "        \"knn\": int(KNN),\n",
        "        \"emb_dims\": int(EMB_DIMS),\n",
        "        \"use_col\": USE_COL,\n",
        "        \"min_count\": float(MIN_COUNT),\n",
        "        \"verbverb_mode\": VERBVERB_MODE,\n",
        "        \"weight_transform\": WEIGHT_TRANSFORM,\n",
        "        \"V\": int(V),\n",
        "    }\n",
        "    blob = json.dumps(obj, sort_keys=True).encode(\"utf-8\")\n",
        "    return hashlib.md5(blob).hexdigest(), obj\n",
        "\n",
        "fp, fp_obj = fingerprint()\n",
        "\n",
        "Z = None\n",
        "active_ids = None\n",
        "\n",
        "if embedding_cache_npz.exists() and embedding_cache_meta.exists():\n",
        "    try:\n",
        "        meta = json.loads(embedding_cache_meta.read_text(encoding=\"utf-8\"))\n",
        "        if meta.get(\"fingerprint\") == fp:\n",
        "            npz = np.load(embedding_cache_npz, allow_pickle=False)\n",
        "            active_ids = npz[\"active_ids\"].astype(np.int32)\n",
        "            Z = npz[\"Z\"].astype(np.float32)\n",
        "            print(\"Loaded cached embedding:\", embedding_cache_npz, \"Z:\", Z.shape)\n",
        "    except Exception:\n",
        "        Z = None\n",
        "\n",
        "if Z is None:\n",
        "    print(\"Computing embedding on CPU...\")\n",
        "\n",
        "    active = X.getnnz(axis=1) > 0\n",
        "    active_ids = np.where(active)[0].astype(np.int32)\n",
        "    X_act = X[active]\n",
        "    n = X_act.shape[0]\n",
        "    if n < 5:\n",
        "        raise RuntimeError(f\"Too few active verbs: {n}\")\n",
        "\n",
        "    print(\"Active verbs:\", n)\n",
        "\n",
        "    Xn = normalize(X_act, axis=1)\n",
        "\n",
        "    k_eff = int(min(int(KNN), n - 1))\n",
        "    if k_eff < 1:\n",
        "        raise RuntimeError(\"k_eff < 1\")\n",
        "\n",
        "    nn = NNDescent(\n",
        "        Xn,\n",
        "        n_neighbors=k_eff + 1,\n",
        "        metric=\"cosine\",\n",
        "        random_state=0,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    knn_idx, knn_dist = nn.neighbor_graph\n",
        "    knn_sim = 1.0 - knn_dist\n",
        "\n",
        "    I = np.repeat(np.arange(n, dtype=np.int32), k_eff)\n",
        "    J = knn_idx[:, 1:k_eff+1].reshape(-1).astype(np.int32)\n",
        "    S = knn_sim[:, 1:k_eff+1].reshape(-1).astype(np.float32)\n",
        "\n",
        "    keep = S > 0.0\n",
        "    I, J, S = I[keep], J[keep], S[keep]\n",
        "\n",
        "    W = sp.coo_matrix((S, (I, J)), shape=(n, n), dtype=np.float32).tocsr()\n",
        "    W = (W + W.T).tocsr()\n",
        "    W.sum_duplicates()\n",
        "\n",
        "    W_emb = (W + sp.eye(n, dtype=np.float32) * 1e-6).tocsr()\n",
        "\n",
        "    emb_dim_used = int(min(int(EMB_DIMS), n - 2))\n",
        "    if emb_dim_used < 2:\n",
        "        raise RuntimeError(f\"emb_dim_used too small: {emb_dim_used}\")\n",
        "\n",
        "    Z = SpectralEmbedding(\n",
        "        n_components=emb_dim_used,\n",
        "        affinity=\"precomputed\",\n",
        "        random_state=0,\n",
        "    ).fit_transform(W_emb)\n",
        "\n",
        "    Z = normalize(Z, axis=1).astype(np.float32)\n",
        "\n",
        "    np.savez_compressed(embedding_cache_npz, active_ids=active_ids, Z=Z)\n",
        "    embedding_cache_meta.write_text(json.dumps({\"fingerprint\": fp, \"config\": fp_obj}, ensure_ascii=False, indent=2),\n",
        "                                    encoding=\"utf-8\")\n",
        "    print(\"Saved embedding cache:\", embedding_cache_npz)\n",
        "\n",
        "print(\"Embedding ready. Z:\", Z.shape, \"active_ids:\", active_ids.shape)\n",
        "\n",
        "dfc = pd.read_csv(CLUSTERS_CSV, encoding=\"utf-8-sig\")\n",
        "if \"verb\" not in dfc.columns:\n",
        "    raise ValueError(\"Cluster CSV must contain 'verb' column\")\n",
        "if SEED_COL not in dfc.columns:\n",
        "    raise ValueError(f\"{SEED_COL} not found in cluster CSV\")\n",
        "\n",
        "dfc[\"verb\"] = dfc[\"verb\"].astype(str).str.strip()\n",
        "\n",
        "dfc_map = dict(zip(dfc[\"verb\"].tolist(), pd.to_numeric(dfc[SEED_COL], errors=\"coerce\").fillna(-1).astype(int).tolist()))\n",
        "labels_full = np.array([dfc_map.get(v, -1) for v in verbs], dtype=np.int32)\n",
        "\n",
        "labels_active = labels_full[active_ids]\n",
        "valid = labels_active >= 0\n",
        "if not np.any(valid):\n",
        "    raise RuntimeError(\"No labeled active verbs found (all -1?)\")\n",
        "\n",
        "K_detected = int(labels_active[valid].max() + 1)\n",
        "print(\"Detected K (from labels):\", K_detected)\n",
        "\n",
        "verbs_active = np.array([verbs[i] for i in active_ids], dtype=object)\n",
        "\n",
        "centroids = np.zeros((K_detected, Z.shape[1]), dtype=np.float32)\n",
        "sizes = np.zeros((K_detected,), dtype=np.int32)\n",
        "\n",
        "rows = []\n",
        "for c in range(K_detected):\n",
        "    idx = np.where(labels_active == c)[0]\n",
        "    sizes[c] = int(idx.size)\n",
        "\n",
        "    if idx.size == 0:\n",
        "        centroids[c] = 0.0\n",
        "        top_verbs = \"\"\n",
        "    else:\n",
        "        cen = Z[idx].mean(axis=0)\n",
        "        cen = cen / (np.linalg.norm(cen) + 1e-12)\n",
        "        centroids[c] = cen.astype(np.float32)\n",
        "\n",
        "        sims = Z @ centroids[c]  # cosine sim\n",
        "        sims_in = sims[idx]\n",
        "        top_local = idx[np.argsort(-sims_in)[:TOP_N]]\n",
        "        top_verbs = \", \".join(verbs_active[top_local].tolist())\n",
        "\n",
        "    rows.append({\n",
        "        \"cluster\": int(c),\n",
        "        \"size\": int(sizes[c]),\n",
        "        \"top_verbs\": top_verbs\n",
        "    })\n",
        "\n",
        "summary = pd.DataFrame(rows).sort_values(\"size\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "np.savez_compressed(centroids_npz, centroids=centroids, sizes=sizes, clusters=np.arange(K_detected, dtype=np.int32))\n",
        "summary.to_csv(centroids_summary_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"Saved centroids:\", centroids_npz)\n",
        "print(\"Saved summary:\", centroids_summary_csv)\n",
        "summary.head(20)"
      ],
      "metadata": {
        "id": "7Ii29UXcbfy0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}